# Camada Silver - Data Lakehouse Pipeline

## ðŸ“‹ VisÃ£o Geral

A **Camada Silver** Ã© responsÃ¡vel por transformar dados brutos (Bronze) em dados limpos, padronizados e prontos para anÃ¡lise. Esta camada implementa processos de **Data Cleansing** e **Data Standardization**.

## ðŸ—ï¸ Arquitetura Implementada

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SILVER LAYER ARCHITECTURE                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   Bronze     â”‚â”€â”€â”€â–¶â”‚  Silver Lambda   â”‚â”€â”€â”€â–¶â”‚    Silver    â”‚  â”‚
â”‚  â”‚   Bucket     â”‚    â”‚  (Cleansing)     â”‚    â”‚    Bucket    â”‚  â”‚
â”‚  â”‚  (Parquet)   â”‚    â”‚                  â”‚    â”‚  (Parquet)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                     â”‚                      â”‚          â”‚
â”‚         â”‚                     â”‚                      â”‚          â”‚
â”‚         â–¼                     â–¼                      â–¼          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ EventBridge  â”‚    â”‚   CloudWatch    â”‚    â”‚ Glue Crawler â”‚  â”‚
â”‚  â”‚  Scheduler   â”‚    â”‚      Logs       â”‚    â”‚   (Daily)    â”‚  â”‚
â”‚  â”‚  (Hourly)    â”‚    â”‚                 â”‚    â”‚              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                      â”‚          â”‚
â”‚                                                      â–¼          â”‚
â”‚                                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚                                              â”‚ Glue Catalog â”‚  â”‚
â”‚                                              â”‚ (Metadata)   â”‚  â”‚
â”‚                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1. **AWS Lambda Function - `cleansing-function`**

**Responsabilidades:**
- Ler dados particionados do Bronze bucket
- Aplicar transformaÃ§Ãµes de limpeza e padronizaÃ§Ã£o
- Escrever dados limpos no Silver bucket (formato Parquet particionado)

**ConfiguraÃ§Ã£o:**
- **Runtime:** Python 3.9
- **Memory:** 1024 MB
- **Timeout:** 600 segundos (10 minutos)
- **Lambda Layer:** pandas + pyarrow (compartilhado)

**VariÃ¡veis de Ambiente:**
```hcl
BRONZE_BUCKET_NAME    = "datalake-pipeline-bronze-dev"
SILVER_BUCKET_NAME    = "datalake-pipeline-silver-dev"
GLUE_DATABASE_NAME    = "datalake-pipeline-catalog-dev"
LOG_LEVEL             = "INFO"
PARTITION_COLUMNS     = "partition_year,partition_month,partition_day"
ENABLE_DATA_QUALITY   = "true"
```

### 2. **IAM Role - `silver-lambda-role`**

**PermissÃµes Concedidas:**

**S3 Access:**
- **Read from Bronze:** `s3:GetObject`, `s3:ListBucket` em `bronze-bucket/*`
- **Write to Silver:** `s3:PutObject`, `s3:PutObjectAcl`, `s3:DeleteObject` em `silver-bucket/*`

**CloudWatch Logs:**
- `logs:CreateLogGroup`
- `logs:CreateLogStream`
- `logs:PutLogEvents`

**Glue Catalog (Read):**
- `glue:GetDatabase`
- `glue:GetTable`
- `glue:GetPartitions`
- `glue:BatchGetPartition`

### 3. **Amazon EventBridge Scheduler**

**ConfiguraÃ§Ã£o:**
- **Nome:** `silver-etl-schedule`
- **Schedule:** `rate(1 hour)` - Executa a cada hora
- **Target:** Lambda `cleansing-function`
- **Retry Policy:** 2 tentativas, janela de 1 hora

**Payload de Entrada:**
```json
{
  "source_bucket": "datalake-pipeline-bronze-dev",
  "target_bucket": "datalake-pipeline-silver-dev",
  "table_name": "bronze",
  "triggered_by": "eventbridge-scheduler"
}
```

### 4. **AWS Glue Crawler - `silver_data_crawler`**

**ConfiguraÃ§Ã£o:**
- **Schedule:** `cron(0 1 * * ? *)` - Diariamente Ã s 01:00 UTC
- **Target Path:** `s3://silver-bucket/silver/`
- **Database:** `analytics_db` (catÃ¡logo existente)
- **Partition Detection:** Habilitado (detecta `partition_year`, `partition_month`, `partition_day`)

**Schema Change Policy:**
- **Delete Behavior:** LOG (registra exclusÃµes)
- **Update Behavior:** UPDATE_IN_DATABASE (atualiza schema automaticamente)

**Table Behavior:**
- **Add/Update Behavior:** MergeNewColumns (adiciona novas colunas sem sobrescrever)

## ðŸ”„ Fluxo de Dados

### Passo a Passo:

1. **Trigger Agendado (EventBridge)**
   - A cada hora, o EventBridge Scheduler aciona a Lambda de Cleansing

2. **Leitura do Bronze**
   - Lambda lÃª as partiÃ§Ãµes mais recentes do Bronze bucket
   - Formato: `bronze/partition_year=YYYY/partition_month=MM/partition_day=DD/*.parquet`

3. **TransformaÃ§Ãµes Aplicadas**
   - **Flatten:** Estruturas jÃ¡ achatadas no Bronze (JSON normalize)
   - **PadronizaÃ§Ã£o:** Nomes de colunas â†’ `lowercase_snake_case`
   - **DeduplicaÃ§Ã£o:** Remove registros duplicados exatos
   - **ValidaÃ§Ã£o:** Garante tipos de dados corretos (float64 para numÃ©ricos)
   - **Enriquecimento:** Adiciona metadados:
     - `silver_processing_timestamp`
     - `silver_processing_date`
     - `data_quality_score`
   - **Particionamento:** Adiciona colunas `event_year`, `event_month`, `event_day`

4. **Escrita no Silver**
   - Dados limpos sÃ£o escritos no Silver bucket
   - Formato: `silver/partition_year=YYYY/partition_month=MM/partition_day=DD/car_telemetry_*.parquet`
   - CompressÃ£o: Snappy
   - Metadados S3: Incluem contagem de registros, timestamp, etc.

5. **CatalogaÃ§Ã£o (Glue Crawler)**
   - Diariamente Ã s 01:00 UTC, o Crawler escaneia o Silver bucket
   - Detecta novas partiÃ§Ãµes automaticamente
   - Atualiza o Glue Data Catalog com schema e metadados
   - Torna os dados consultÃ¡veis via Amazon Athena

## ðŸ“Š Estrutura de Dados

### Bronze â†’ Silver Transformation

**Bronze (Input):**
```
carchassis, model, year, modelyear, manufacturer, ...
metrics.trip.tripfuelliters, metrics.trip.tripmileage, ...
ingestion_timestamp, source_file, source_bucket
```

**Silver (Output):**
```
carchassis, model, year, modelyear, manufacturer, ...
metrics_trip_tripfuelliters, metrics_trip_tripmileage, ...  â† Standardized names
ingestion_timestamp, source_file, source_bucket
silver_processing_timestamp â† New metadata
silver_processing_date â† New metadata
data_quality_score â† New metadata
event_year, event_month, event_day â† Partition columns (NOT in data)
```

### PartiÃ§Ãµes no Silver

```
s3://silver-bucket/
â””â”€â”€ silver/
    â””â”€â”€ partition_year=2025/
        â””â”€â”€ partition_month=10/
            â””â”€â”€ partition_day=30/
                â”œâ”€â”€ car_telemetry_20251030_120000.parquet
                â”œâ”€â”€ car_telemetry_20251030_130000.parquet
                â””â”€â”€ car_telemetry_20251030_140000.parquet
```

## ðŸš€ Como Implantar

### 1. Validar VariÃ¡veis (terraform/variables.tf)

```hcl
# Ajustar conforme necessÃ¡rio
variable "silver_lambda_memory" {
  default = 1024  # MB
}

variable "silver_lambda_timeout" {
  default = 600  # 10 minutos
}

variable "silver_etl_schedule" {
  default = "rate(1 hour)"  # A cada hora
}

variable "silver_crawler_schedule" {
  default = "cron(0 1 * * ? *)"  # 01:00 UTC diariamente
}
```

### 2. Aplicar Terraform

```bash
cd terraform
terraform init
terraform plan
terraform apply
```

### 3. Verificar Recursos Criados

```bash
# Lambda Function
aws lambda get-function --function-name datalake-pipeline-cleansing-silver-dev

# EventBridge Scheduler
aws scheduler get-schedule --name datalake-pipeline-silver-etl-schedule-dev

# Glue Crawler
aws glue get-crawler --name datalake-pipeline-silver-crawler-dev
```

### 4. Testar Manualmente

**Invocar Lambda manualmente:**
```bash
aws lambda invoke \
  --function-name datalake-pipeline-cleansing-silver-dev \
  --payload '{"source_bucket":"datalake-pipeline-bronze-dev","target_bucket":"datalake-pipeline-silver-dev"}' \
  response.json

cat response.json
```

**Executar Crawler manualmente:**
```bash
aws glue start-crawler --name datalake-pipeline-silver-crawler-dev

# Verificar status
aws glue get-crawler --name datalake-pipeline-silver-crawler-dev
```

## ðŸ“ˆ Monitoramento

### CloudWatch Logs

```bash
# Ver logs da Lambda
aws logs tail /aws/lambda/datalake-pipeline-cleansing-silver-dev --follow

# Ver logs do Crawler
aws logs tail /aws-glue/crawlers --filter-pattern "silver-crawler" --follow
```

### MÃ©tricas CloudWatch

- **Lambda:**
  - Invocations
  - Duration
  - Errors
  - Throttles

- **Scheduler:**
  - InvocationAttemptCount
  - InvocationSuccessCount
  - InvocationFailureCount

## ðŸŽ¯ BenefÃ­cios da Camada Silver

### 1. **Dados Limpos e ConfiÃ¡veis**
- RemoÃ§Ã£o de duplicatas
- ValidaÃ§Ã£o de tipos de dados
- Tratamento de valores nulos

### 2. **PadronizaÃ§Ã£o**
- Nomes de colunas consistentes
- Formato uniforme (Parquet)
- Estrutura previsÃ­vel

### 3. **Performance Otimizada**
- Particionamento por data
- CompressÃ£o Snappy
- Formato colunar (Parquet)

### 4. **Rastreabilidade**
- Metadados de processamento
- Score de qualidade de dados
- Timestamps de transformaÃ§Ã£o

### 5. **Pronto para AnÃ¡lise**
- Dados estruturados
- Schema catalogado no Glue
- ConsultÃ¡vel via Athena

## ðŸ”§ CustomizaÃ§Ãµes Comuns

### Alterar FrequÃªncia do ETL

**Executar a cada 30 minutos:**
```hcl
variable "silver_etl_schedule" {
  default = "rate(30 minutes)"
}
```

**Executar diariamente Ã s 02:00 UTC:**
```hcl
variable "silver_etl_schedule" {
  default = "cron(0 2 * * ? *)"
}
```

### Aumentar Recursos da Lambda

```hcl
variable "silver_lambda_memory" {
  default = 2048  # 2 GB
}

variable "silver_lambda_timeout" {
  default = 900  # 15 minutos
}
```

### Adicionar TransformaÃ§Ãµes Customizadas

Edite `lambdas/silver/silver_etl.py` e adicione lÃ³gica na funÃ§Ã£o `apply_transformations()`:

```python
def apply_transformations(df):
    # ... existing transformations ...
    
    # Custom transformation example
    df['vehicle_age'] = 2025 - df['year']
    
    # Add business logic
    df['price_category'] = df['market_currentprice'].apply(
        lambda x: 'High' if x > 50000 else 'Medium' if x > 25000 else 'Low'
    )
    
    return df
```

## ðŸ“š ReferÃªncias

- [AWS Lambda Documentation](https://docs.aws.amazon.com/lambda/)
- [Amazon EventBridge Scheduler](https://docs.aws.amazon.com/scheduler/)
- [AWS Glue Crawler](https://docs.aws.amazon.com/glue/latest/dg/add-crawler.html)
- [Apache Parquet](https://parquet.apache.org/)
- [Pandas Documentation](https://pandas.pydata.org/)

## ðŸŽ‰ PrÃ³ximos Passos

1. âœ… **Silver Layer Implementada**
2. ðŸš§ **Implementar Gold Layer** (AgregaÃ§Ãµes e Analytics)
3. ðŸš§ **Adicionar Data Quality Checks**
4. ðŸš§ **Implementar Alertas CloudWatch**
5. ðŸš§ **Dashboard de Monitoramento**

---

**Criado por:** Data Engineering Team  
**Ãšltima AtualizaÃ§Ã£o:** 2025-10-30  
**VersÃ£o:** 1.0.0
