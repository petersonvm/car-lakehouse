# AWS Glue Job - Silver Layer Consolidation com Upsert

## üìã Vis√£o Geral

Script PySpark completo para AWS Glue ETL que implementa:
- ‚úÖ Leitura incremental com **Glue Job Bookmarks**
- ‚úÖ Transforma√ß√µes Silver (flatten, cleanse, enrich, partition)
- ‚úÖ **Consolida√ß√£o com Upsert** (elimina duplicatas)
- ‚úÖ **Dynamic Partition Overwrite** (atualiza apenas parti√ß√µes afetadas)

## üéØ Problema Resolvido

**Antes (Lambda):**
- Cada execu√ß√£o **anexava** dados sem verificar duplicatas
- Mesmo `carChassis` + `event_day` podia ter m√∫ltiplos registros
- Queries retornavam dados duplicados

**Depois (Glue Job):**
- Carrega dados novos **+** dados existentes
- Aplica deduplica√ß√£o com **Window function**
- Sobrescreve apenas parti√ß√µes afetadas
- **1 registro √∫nico** por `carChassis` + `event_day`

## üîë Par√¢metros do Job

O script espera os seguintes par√¢metros ao criar o Glue Job:

| Par√¢metro | Descri√ß√£o | Exemplo |
|-----------|-----------|---------|
| `bronze_database` | Database do Bronze no Glue Catalog | `datalake-pipeline-catalog-dev` |
| `bronze_table` | Tabela Bronze | `bronze_ingest_year_2025` |
| `silver_database` | Database do Silver no Glue Catalog | `datalake-pipeline-catalog-dev` |
| `silver_table` | Tabela Silver | `silver_car_telemetry` |
| `silver_bucket` | Bucket S3 do Silver | `datalake-pipeline-silver-dev` |
| `silver_path` | Path dentro do bucket | `car_telemetry/` |

## üîÑ Fluxo de Execu√ß√£o

### 1Ô∏è‚É£ **Leitura Incremental (Bookmarks)**
```python
bronze_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(
    database=args['bronze_database'],
    table_name=args['bronze_table'],
    transformation_ctx="bronze_source",  # ‚Üê Bookmark tracking
)
```
- ‚úÖ Processa **apenas arquivos novos** no Bronze
- ‚úÖ Rastreia arquivos j√° processados via `transformation_ctx`
- ‚úÖ Evita reprocessamento desnecess√°rio

### 2Ô∏è‚É£ **Transforma√ß√µes Silver (5 Etapas)**

#### A. Achatamento (Flattening)
```python
# Achatar struct 'metrics'
df_flattened = df.select(
    "*",
    F.col("metrics.engineTempCelsius").alias("metrics_engineTempCelsius"),
    F.col("metrics.trip.tripMileage").alias("metrics_trip_tripMileage"),
    ...
).drop("metrics")
```
- `metrics` ‚Üí `metrics_engineTempCelsius`, `metrics_fuelAvailableLiters`, etc.
- `metrics.trip` ‚Üí `metrics_trip_tripMileage`, `metrics_trip_tripFuelLiters`, etc.
- `carInsurance` ‚Üí `carInsurance_number`, `carInsurance_provider`, etc.
- `market` ‚Üí `market_currentPrice`, `market_location`, etc.

#### B. Limpeza (Cleansing)
```python
df_clean = df.withColumn("Manufacturer", F.initcap(F.col("Manufacturer")))
              .withColumn("color", F.lower(F.col("color")))
```
- `Manufacturer`: `hyundai` ‚Üí `Hyundai` (Title Case)
- `color`: `Blue` ‚Üí `blue` (lowercase)

#### C. Convers√£o de Tipos
```python
df_typed = df.withColumn(
    "metrics_metricTimestamp",
    F.to_timestamp(F.col("metrics_metricTimestamp"), "EEE, dd MMM yyyy HH:mm:ss")
)
```
- Timestamps: String ‚Üí `timestamp` type
- Datas: String ‚Üí `date` type

#### D. Enriquecimento (Enrichment)
```python
df_enriched = df.withColumn(
    "metrics_fuel_level_percentage",
    F.round((F.col("metrics_fuelAvailableLiters") / F.col("fuelCapacityLiters")) * 100, 2)
).withColumn(
    "metrics_trip_km_per_liter",
    F.round(F.col("metrics_trip_tripMileage") / F.col("metrics_trip_tripFuelLiters"), 2)
)
```
- `metrics_fuel_level_percentage`: (fuelAvailable / fuelCapacity) √ó 100
- `metrics_trip_km_per_liter`: tripMileage / tripFuelLiters

#### E. Particionamento (Event-based)
```python
df_partitioned = df.withColumn("event_year", F.year(F.col("metrics_metricTimestamp")))
                   .withColumn("event_month", F.month(F.col("metrics_metricTimestamp")))
                   .withColumn("event_day", F.dayofmonth(F.col("metrics_metricTimestamp")))
```
- Parti√ß√µes baseadas na **data do evento** (n√£o da ingest√£o)
- Formato: `event_year=2025/event_month=10/event_day=29/`

### 3Ô∏è‚É£ **Consolida√ß√£o (Upsert/Deduplica√ß√£o)**

Este √© o **cora√ß√£o do Job** que resolve o problema de duplicatas.

#### Passo A: Carregar Dados Existentes
```python
df_silver_existing = glueContext.create_dynamic_frame.from_catalog(
    database=args['silver_database'],
    table_name=args['silver_table']
).toDF()
```

#### Passo B: Unir Novos + Existentes
```python
df_union = df_silver_new.unionByName(df_silver_existing)
```

#### Passo C: Aplicar Deduplica√ß√£o (Window Function)
```python
window_spec = Window.partitionBy(
    "carChassis",
    "event_year",
    "event_month",
    "event_day"
).orderBy(
    F.col("currentMileage").desc()  # ‚Üê Regra de preced√™ncia
)

df_deduplicated = df_union.withColumn(
    "row_num",
    F.row_number().over(window_spec)
).filter(
    F.col("row_num") == 1
).drop("row_num")
```

**L√≥gica de Deduplica√ß√£o:**
- **Chave de neg√≥cio**: `carChassis` + `event_year` + `event_month` + `event_day`
- **Regra de preced√™ncia**: `currentMileage DESC` (maior milhagem = mais recente)
- **Resultado**: Mant√©m apenas 1 registro por combina√ß√£o de chave

**Exemplo:**
```
ANTES DA DEDUPLICA√á√ÉO:
carChassis          | currentMileage | event_day | row_num
5ifRW...            | 4321           | 29        | 2  ‚Üê descartado
5ifRW...            | 8500           | 29        | 1  ‚Üê mantido ‚úÖ

DEPOIS DA DEDUPLICA√á√ÉO:
carChassis          | currentMileage | event_day
5ifRW...            | 8500           | 29        ‚Üê √∫nico registro
```

### 4Ô∏è‚É£ **Escrita (Dynamic Partition Overwrite)**

```python
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

glueContext.write_dynamic_frame.from_options(
    frame=silver_output_dynamic_frame,
    connection_type="s3",
    connection_options={
        "path": "s3://silver-bucket/car_telemetry/",
        "partitionKeys": ["event_year", "event_month", "event_day"]
    },
    format="parquet"
)
```

**Dynamic Partition Overwrite:**
- ‚úÖ Sobrescreve **apenas** parti√ß√µes afetadas (ex: `event_day=29`)
- ‚úÖ **Preserva** todas as outras parti√ß√µes intactas
- ‚úÖ Evita reescrever o dataset inteiro

**Exemplo:**
```
PARTI√á√ïES ANTES:
‚îú‚îÄ‚îÄ event_day=28/  ‚Üê intocado ‚úÖ
‚îú‚îÄ‚îÄ event_day=29/  ‚Üê sobrescrito (consolidado) üîÑ
‚îî‚îÄ‚îÄ event_day=30/  ‚Üê intocado ‚úÖ
```

## üöÄ Como Usar

### 1. Criar o Glue Job no Terraform

Adicione ao seu `terraform/glue.tf`:

```hcl
resource "aws_glue_job" "silver_consolidation" {
  name     = "datalake-pipeline-silver-consolidation-dev"
  role_arn = aws_iam_role.glue_job_role.arn
  
  command {
    name            = "glueetl"
    script_location = "s3://${aws_s3_bucket.scripts.id}/glue_jobs/silver_consolidation_job.py"
    python_version  = "3"
  }
  
  glue_version = "4.0"
  
  default_arguments = {
    "--job-language"                     = "python"
    "--job-bookmark-option"              = "job-bookmark-enable"  # ‚Üê Bookmarks habilitados
    "--enable-metrics"                   = "true"
    "--enable-continuous-cloudwatch-log" = "true"
    "--TempDir"                          = "s3://${aws_s3_bucket.scripts.id}/temp/"
    
    # Par√¢metros personalizados
    "--bronze_database"  = aws_glue_catalog_database.data_lake_database.name
    "--bronze_table"     = "bronze_ingest_year_2025"
    "--silver_database"  = aws_glue_catalog_database.data_lake_database.name
    "--silver_table"     = "silver_car_telemetry"
    "--silver_bucket"    = aws_s3_bucket.data_lake["silver"].id
    "--silver_path"      = "car_telemetry/"
  }
  
  max_retries      = 0
  timeout          = 60  # 60 minutos
  worker_type      = "G.1X"
  number_of_workers = 2
}
```

### 2. Fazer Upload do Script

```bash
aws s3 cp glue_jobs/silver_consolidation_job.py \
  s3://your-scripts-bucket/glue_jobs/
```

### 3. Executar o Job

**Manualmente:**
```bash
aws glue start-job-run --job-name datalake-pipeline-silver-consolidation-dev
```

**Agendado (via Trigger):**
```hcl
resource "aws_glue_trigger" "silver_consolidation_schedule" {
  name     = "silver-consolidation-daily"
  type     = "SCHEDULED"
  schedule = "cron(0 2 * * ? *)"  # Diariamente √†s 2 AM UTC
  
  actions {
    job_name = aws_glue_job.silver_consolidation.name
  }
}
```

## üìä Exemplo de Execu√ß√£o

```
üöÄ Job iniciado: silver-consolidation-dev
üìÖ Timestamp: 2025-10-30T15:45:00

üì• ETAPA 1: Leitura de dados novos do Bronze...
   ‚úÖ Registros novos encontrados: 1

üîÑ ETAPA 2: Aplicando transforma√ß√µes Silver...
   üîπ 1/5: Achatando estruturas aninhadas (structs)...
      ‚úÖ 36 colunas ap√≥s achatamento
   üîπ 2/5: Aplicando limpeza e padroniza√ß√£o...
      ‚úÖ Manufacturer ‚Üí Title Case, color ‚Üí lowercase
   üîπ 3/5: Convertendo tipos de dados...
      ‚úÖ Timestamps e datas convertidos
   üîπ 4/5: Calculando m√©tricas enriquecidas...
      ‚úÖ M√©tricas calculadas
   üîπ 5/5: Criando colunas de parti√ß√£o...
      ‚úÖ Parti√ß√µes criadas

üìö ETAPA 3: Carregando dados existentes do Silver...
   ‚úÖ Registros existentes: 1

üîÄ ETAPA 4: Consolidando dados (Upsert/Deduplica√ß√£o)...
   üìä Total antes da deduplica√ß√£o: 2
   ‚úÖ Total ap√≥s deduplica√ß√£o: 1
   üóëÔ∏è  Duplicatas removidas: 1

üíæ ETAPA 5: Escrevendo dados consolidados no Silver...
   ‚úÖ Dados escritos com sucesso!
   üì¶ Registros finais: 1
   üìÇ Parti√ß√µes escritas: event_year=2025/event_month=10/event_day=29

‚úÖ JOB CONCLU√çDO COM SUCESSO!
```

## üß™ Valida√ß√£o no Athena

Ap√≥s executar o Job, valide que n√£o h√° duplicatas:

```sql
-- Verificar duplicatas (deve retornar 0)
SELECT carChassis, event_year, event_month, event_day, COUNT(*) as count
FROM silver_car_telemetry
GROUP BY carChassis, event_year, event_month, event_day
HAVING COUNT(*) > 1;

-- Verificar registro consolidado (deve mostrar currentMileage mais alto)
SELECT carChassis, currentMileage, metrics_metricTimestamp, event_day
FROM silver_car_telemetry
WHERE event_year = '2025' AND event_month = '10' AND event_day = '29';
```

## ‚ö° Performance

**Otimiza√ß√µes Implementadas:**
- ‚úÖ **Bookmarks**: Processa apenas dados novos
- ‚úÖ **Predicate Pushdown**: Filtros aplicados no Data Catalog
- ‚úÖ **Dynamic Partitioning**: Escreve apenas parti√ß√µes modificadas
- ‚úÖ **Compression**: Parquet com Snappy
- ‚úÖ **Columnar Format**: Queries otimizadas

**Escalabilidade:**
- Worker Type: `G.1X` (4 vCPU, 16 GB RAM)
- Workers: 2 (ajuste conforme volume de dados)
- Timeout: 60 minutos

## üéØ Pr√≥ximos Passos

1. **Criar Glue Job no Terraform**
2. **Fazer upload do script para S3**
3. **Executar job manualmente** para testar
4. **Executar Glue Crawler** no Silver para atualizar cat√°logo
5. **Validar no Athena** (verificar aus√™ncia de duplicatas)
6. **Configurar trigger agendado** (di√°rio, hor√°rio, etc.)

## üìù Notas Importantes

- ‚ö†Ô∏è **Primeira Execu√ß√£o**: Se tabela Silver n√£o existir, apenas escreve dados novos
- ‚ö†Ô∏è **Regra de Deduplica√ß√£o**: Ajuste `orderBy()` conforme sua regra de neg√≥cio
- ‚ö†Ô∏è **Bookmarks**: N√£o delete manualmente - use reset via AWS CLI se necess√°rio
- ‚ö†Ô∏è **IAM Permissions**: Role do Glue precisa de `s3:GetObject`, `s3:PutObject`, `glue:GetTable`

---

**Autor**: Sistema de Data Lakehouse  
**Data**: 2025-10-30  
**Vers√£o**: 1.0
