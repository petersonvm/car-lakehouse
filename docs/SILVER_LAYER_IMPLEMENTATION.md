# ü•à Implementa√ß√£o da Camada Silver - Pipeline Bronze-to-Silver

**Data de Cria√ß√£o:** 30 de Outubro de 2025  
**Vers√£o:** 1.0.0  
**Status:** ‚úÖ Implementado e Documentado

---

## üìã √çndice

1. [Vis√£o Geral](#vis√£o-geral)
2. [Arquitetura do Pipeline](#arquitetura-do-pipeline)
3. [Componentes Implementados](#componentes-implementados)
4. [L√≥gica de Transforma√ß√£o](#l√≥gica-de-transforma√ß√£o)
5. [Infraestrutura Terraform](#infraestrutura-terraform)
6. [Particionamento Baseado em Eventos](#particionamento-baseado-em-eventos)
7. [Deployment e Configura√ß√£o](#deployment-e-configura√ß√£o)
8. [Testes e Valida√ß√£o](#testes-e-valida√ß√£o)
9. [Queries Athena (Exemplos)](#queries-athena-exemplos)
10. [Troubleshooting](#troubleshooting)
11. [Pr√≥ximos Passos](#pr√≥ximos-passos)

---

## üéØ Vis√£o Geral

### Objetivo da Camada Silver

A **Camada Silver** (tamb√©m chamada de "Refined Layer") √© respons√°vel por transformar os dados brutos preservados no Bronze em um formato otimizado para an√°lises:

- ‚úÖ **Achatamento:** Converte estruturas aninhadas (`struct` columns) em colunas planas
- ‚úÖ **Limpeza:** Padroniza formatos (Title Case, lowercase, trim)
- ‚úÖ **Convers√£o de Tipos:** Transforma strings em datetime/date conforme necess√°rio
- ‚úÖ **Enriquecimento:** Calcula m√©tricas derivadas (percentuais, taxas, KPIs)
- ‚úÖ **Particionamento Inteligente:** Organiza por data do evento (n√£o de ingest√£o)

### Diferen√ßas: Bronze vs Silver

| Aspecto | Bronze Layer | Silver Layer |
|---------|-------------|--------------|
| **Formato** | Parquet com structs aninhados | Parquet achatado (flat) |
| **Particionamento** | Data de ingest√£o (`ingest_year/month/day`) | Data do evento (`event_year/month/day`) |
| **Schema** | Preserva estrutura original JSON | Schema normalizado e limpo |
| **Dados** | C√≥pia fiel da fonte | Transformado, limpo e enriquecido |
| **Uso** | Auditoria, troubleshooting, reprocessamento | Analytics, BI, Machine Learning |
| **Queries** | Complexas (dot notation para structs) | Simples (colunas planas) |

---

## üèóÔ∏è Arquitetura do Pipeline

### Fluxo End-to-End

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Landing   ‚îÇ  JSON/CSV files uploaded by users
‚îÇ   Bucket    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îÇ S3 Event (*.json, *.csv)
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Ingestion Lambda    ‚îÇ  Converts to Parquet (preserves structs)
‚îÇ (Bronze Layer)      ‚îÇ  Partitions by ingest_date
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ Writes Parquet
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Bronze Bucket     ‚îÇ  Parquet with nested structures
‚îÇ /bronze/car_data/   ‚îÇ  Partitions: ingest_year/month/day
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ S3 Event (*.parquet)
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cleansing Lambda    ‚îÇ  ‚ú® NEW - Silver Layer
‚îÇ (Silver Layer)      ‚îÇ  1. Flatten structs
‚îÇ                     ‚îÇ  2. Cleanse data
‚îÇ                     ‚îÇ  3. Enrich metrics
‚îÇ                     ‚îÇ  4. Partition by event_date
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ Writes Flattened Parquet
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Silver Bucket     ‚îÇ  Parquet with flat columns
‚îÇ /car_telemetry/     ‚îÇ  Partitions: event_year/month/day
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ Daily at 01:00 UTC
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Silver Crawler     ‚îÇ  Catalogs flattened data
‚îÇ  (AWS Glue)         ‚îÇ  Detects event-based partitions
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ Updates Catalog
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Glue Data Catalog  ‚îÇ  Table: silver_car_telemetry
‚îÇ                     ‚îÇ  Queryable via Athena
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚îÇ SQL Queries
           ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Amazon Athena      ‚îÇ  Fast queries on flat data
‚îÇ                     ‚îÇ  No dot notation needed
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Trigger Autom√°tico

**Evento:** S3 ObjectCreated no `bronze-bucket`  
**Filtro:** Apenas arquivos `.parquet`  
**A√ß√£o:** Invoca√ß√£o da `cleansing-function` (Lambda)

---

## üîß Componentes Implementados

### 1. **Lambda Function (Python)**
- **Arquivo:** `lambdas/cleansing/lambda_function.py`
- **Handler:** `lambda_handler`
- **Runtime:** Python 3.9
- **Memory:** 1024 MB (maior que ingestion devido ao processamento)
- **Timeout:** 300 segundos (5 minutos)
- **Lambda Layer:** pandas-pyarrow (compartilhada com ingestion)

### 2. **Terraform Resources**

#### Lambda Function
```hcl
resource "aws_lambda_function" "cleansing" {
  function_name = "datalake-pipeline-cleansing-dev"
  handler       = "lambda_function.lambda_handler"
  runtime       = "python3.9"
  timeout       = 300
  memory_size   = 1024
  layers        = [aws_lambda_layer_version.pandas_pyarrow.arn]
  
  environment {
    variables = {
      SILVER_BUCKET = aws_s3_bucket.data_lake["silver"].id
      # ... outras vari√°veis
    }
  }
}
```

#### S3 Trigger (Bronze Bucket)
```hcl
resource "aws_s3_bucket_notification" "bronze_bucket_notification" {
  bucket = aws_s3_bucket.data_lake["bronze"].id

  lambda_function {
    id                  = "parquet-trigger"
    lambda_function_arn = aws_lambda_function.cleansing.arn
    events              = ["s3:ObjectCreated:*"]
    filter_suffix       = ".parquet"
  }
}
```

#### Glue Crawler (Silver)
```hcl
resource "aws_glue_crawler" "silver_crawler" {
  name          = "datalake-pipeline-silver-crawler-dev"
  table_prefix  = "silver_"
  schedule      = "cron(0 1 * * ? *)"  # Daily at 01:00 UTC
  
  s3_target {
    path = "s3://silver-dev/car_telemetry/"
    exclusions = ["**/_temporary/**", "**/_SUCCESS", "**/.spark*"]
  }
  
  schema_change_policy {
    update_behavior = "UPDATE_IN_DATABASE"
  }
  
  configuration = jsonencode({
    CrawlerOutput = {
      Tables = {
        AddOrUpdateBehavior = "MergeNewColumns"
      }
    }
  })
}
```

---

## üîÑ L√≥gica de Transforma√ß√£o

### Vis√£o Geral das 5 Etapas

```python
def transform_to_silver(df):
    """
    Pipeline de transforma√ß√£o Bronze-to-Silver
    
    INPUT:  DataFrame com colunas struct (metrics, carInsurance, market)
    OUTPUT: DataFrame achatado, limpo e enriquecido
    """
    
    # 1Ô∏è‚É£ ACHATAMENTO (Flatten)
    df_flat = flatten_nested_columns(df)
    
    # 2Ô∏è‚É£ LIMPEZA (Cleansing)
    df_clean = cleanse_data(df_flat)
    
    # 3Ô∏è‚É£ CONVERS√ÉO DE TIPOS
    df_typed = convert_data_types(df_clean)
    
    # 4Ô∏è‚É£ ENRIQUECIMENTO (Enrichment)
    df_enriched = enrich_metrics(df_typed)
    
    # 5Ô∏è‚É£ PARTICIONAMENTO
    df_partitioned = create_event_partitions(df_enriched)
    
    return df_partitioned
```

---

### 1Ô∏è‚É£ Achatamento (Flatten Nested Structures)

**Objetivo:** Converter colunas `struct` em colunas planas com separador `_`

**Exemplo:**

**Bronze (Aninhado):**
```python
{
  "metrics": {
    "engineTempCelsius": 85.5,
    "fuelAvailableLiters": 45.2,
    "trip": {
      "tripMileage": 123.4,
      "tripFuelLiters": 8.5
    }
  }
}
```

**Silver (Achatado):**
```python
{
  "metrics_engineTempCelsius": 85.5,
  "metrics_fuelAvailableLiters": 45.2,
  "metrics_trip_tripMileage": 123.4,
  "metrics_trip_tripFuelLiters": 8.5
}
```

**Implementa√ß√£o:**
```python
def flatten_nested_columns(df):
    """
    Achata structs usando pd.json_normalize com separador '_'
    """
    nested_cols = [col for col in df.columns 
                   if isinstance(df[col].iloc[0], dict)]
    
    df_result = df[[col for col in df.columns if col not in nested_cols]]
    
    for nested_col in nested_cols:
        # Converter dict para DataFrame achatado
        nested_data = df[nested_col].tolist()
        df_nested_flat = pd.json_normalize(nested_data, sep='_')
        
        # Adicionar prefixo com nome da coluna original
        df_nested_flat.columns = [f"{nested_col}_{col}" 
                                  for col in df_nested_flat.columns]
        
        df_result = pd.concat([df_result, df_nested_flat], axis=1)
    
    return df_result
```

---

### 2Ô∏è‚É£ Limpeza (Data Cleansing)

**Objetivo:** Padronizar formatos de texto

**Regras:**

| Campo | Regra | Exemplo |
|-------|-------|---------|
| `manufacturer` | Title Case | "hyundai" ‚Üí "Hyundai" |
| `color` | lowercase | "Blue" ‚Üí "blue" |

**Implementa√ß√£o:**
```python
def cleanse_data(df):
    df_clean = df.copy()
    
    # Manufacturer: Title Case
    if 'manufacturer' in df_clean.columns:
        df_clean['manufacturer'] = df_clean['manufacturer'].str.title()
    
    # color: lowercase
    if 'color' in df_clean.columns:
        df_clean['color'] = df_clean['color'].str.lower()
    
    return df_clean
```

---

### 3Ô∏è‚É£ Convers√£o de Tipos

**Objetivo:** Converter strings em tipos apropriados para an√°lises

**Convers√µes:**

| Campo (Bronze) | Tipo Bronze | Tipo Silver |
|----------------|-------------|-------------|
| `metrics_metricTimestamp` | string (ISO 8601) | datetime |
| `metrics_trip_tripStartTimestamp` | string | datetime |
| `carInsurance_validUntil` | string (YYYY-MM-DD) | date |

**Implementa√ß√£o:**
```python
def convert_data_types(df):
    df_typed = df.copy()
    
    # Timestamps (string ‚Üí datetime)
    timestamp_cols = [
        'metrics_metricTimestamp',
        'metrics_trip_tripStartTimestamp'
    ]
    for col in timestamp_cols:
        if col in df_typed.columns:
            df_typed[col] = pd.to_datetime(df_typed[col], errors='coerce')
    
    # Data de seguro (string ‚Üí date)
    if 'carInsurance_validUntil' in df_typed.columns:
        df_typed['carInsurance_validUntil'] = pd.to_datetime(
            df_typed['carInsurance_validUntil'], errors='coerce'
        ).dt.date
    
    return df_typed
```

---

### 4Ô∏è‚É£ Enriquecimento (Metrics Enrichment)

**Objetivo:** Calcular m√©tricas derivadas para facilitar an√°lises

**M√©tricas Calculadas:**

#### 4.1. **Percentual de Combust√≠vel**
```python
metrics_fuel_level_percentage = (
    metrics_fuelAvailableLiters / fuelCapacityLiters
) * 100
```

**Exemplo:**
- `metrics_fuelAvailableLiters = 45.2`
- `fuelCapacityLiters = 60.0`
- `metrics_fuel_level_percentage = 75.33%`

#### 4.2. **Efici√™ncia de Combust√≠vel (km/litro)**
```python
metrics_trip_km_per_liter = (
    metrics_trip_tripMileage / metrics_trip_tripFuelLiters
)
```

**Exemplo:**
- `metrics_trip_tripMileage = 123.4 km`
- `metrics_trip_tripFuelLiters = 8.5 litros`
- `metrics_trip_km_per_liter = 14.52 km/litro`

**‚ö†Ô∏è Tratamento de Divis√£o por Zero:**
```python
df_enriched['metrics_trip_km_per_liter'] = df.apply(
    lambda row: (
        round(row['metrics_trip_tripMileage'] / 
              row['metrics_trip_tripFuelLiters'], 2)
        if row['metrics_trip_tripFuelLiters'] > 0
        else None  # Retorna NULL se divisor for zero
    ),
    axis=1
)
```

**Implementa√ß√£o Completa:**
```python
def enrich_metrics(df):
    df_enriched = df.copy()
    
    # Percentual de combust√≠vel
    if 'metrics_fuelAvailableLiters' in df.columns and \
       'fuelCapacityLiters' in df.columns:
        df_enriched['metrics_fuel_level_percentage'] = (
            df['metrics_fuelAvailableLiters'] / 
            df['fuelCapacityLiters'] * 100
        ).round(2)
    
    # Efici√™ncia (km/litro) com prote√ß√£o contra divis√£o por zero
    if 'metrics_trip_tripMileage' in df.columns and \
       'metrics_trip_tripFuelLiters' in df.columns:
        df_enriched['metrics_trip_km_per_liter'] = df.apply(
            lambda row: (
                round(row['metrics_trip_tripMileage'] / 
                      row['metrics_trip_tripFuelLiters'], 2)
                if row['metrics_trip_tripFuelLiters'] > 0
                else None
            ),
            axis=1
        )
    
    return df_enriched
```

---

### 5Ô∏è‚É£ Particionamento por Data do Evento

**Objetivo:** Organizar dados pela data do evento (n√£o de ingest√£o)

**Fonte:** Campo `metrics_metricTimestamp` (datetime do evento real)

**Parti√ß√µes Criadas:**
- `event_year` (int): Ano do evento (ex: 2025)
- `event_month` (int): M√™s do evento (ex: 10)
- `event_day` (int): Dia do evento (ex: 30)

**Estrutura S3 Resultante:**
```
s3://silver-bucket/car_telemetry/
‚îú‚îÄ‚îÄ event_year=2025/
‚îÇ   ‚îú‚îÄ‚îÄ event_month=10/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ event_day=28/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_20251028_143022.parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ event_day=29/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ file_20251029_091545.parquet
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ event_day=30/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ file_20251030_120033.parquet
‚îÇ   ‚îî‚îÄ‚îÄ event_month=11/
‚îÇ       ‚îî‚îÄ‚îÄ event_day=01/
‚îÇ           ‚îî‚îÄ‚îÄ file_20251101_080012.parquet
```

**Vantagens do Particionamento por Evento:**
1. ‚úÖ Queries mais eficientes (filtra por data do evento real)
2. ‚úÖ An√°lises temporais precisas (trends, sazonalidade)
3. ‚úÖ Redu√ß√£o de custos (scan apenas parti√ß√µes necess√°rias)

**Implementa√ß√£o:**
```python
def create_event_partitions(df):
    df_part = df.copy()
    
    if 'metrics_metricTimestamp' in df_part.columns:
        # Extrair componentes da data
        df_part['event_year'] = df_part['metrics_metricTimestamp'].dt.year
        df_part['event_month'] = df_part['metrics_metricTimestamp'].dt.month
        df_part['event_day'] = df_part['metrics_metricTimestamp'].dt.day
    else:
        # Fallback: usar data atual
        now = datetime.now()
        df_part['event_year'] = now.year
        df_part['event_month'] = now.month
        df_part['event_day'] = now.day
    
    return df_part
```

---

## üì¶ Infraestrutura Terraform

### Arquivos Modificados

#### 1. `terraform/lambda.tf`
**Adicionado:**
- `aws_lambda_function.cleansing` (fun√ß√£o Silver)
- `aws_lambda_permission.allow_s3_invoke_cleansing` (permiss√£o S3)
- `aws_s3_bucket_notification.bronze_bucket_notification` (trigger)

#### 2. `terraform/variables.tf`
**Adicionado:**
```hcl
variable "cleansing_lambda_config" {
  description = "Configuration for the cleansing Lambda (Silver Layer)"
  type = object({
    function_name    = string
    description      = string
    handler          = string
    runtime          = string
    timeout          = number
    memory_size      = number
    environment_vars = map(string)
  })
  default = {
    function_name = "cleansing"
    description   = "Transforms Bronze Parquet (nested) to Silver (flattened)"
    handler       = "lambda_function.lambda_handler"
    runtime       = "python3.9"
    timeout       = 300
    memory_size   = 1024
    environment_vars = {
      STAGE = "cleansing"
    }
  }
}

variable "cleansing_package_path" {
  description = "Path to the cleansing Lambda deployment package"
  type        = string
  default     = "../assets/cleansing_package.zip"
}
```

#### 3. `terraform/glue.tf`
**Modificado:**
- `aws_glue_crawler.silver_crawler`
  - Path alterado: `silver/` ‚Üí `car_telemetry/`
  - Adicionado `table_prefix = "silver_"`
  - Adicionados `exclusions` para arquivos tempor√°rios
  - Description atualizada

#### 4. `terraform/outputs.tf`
**Adicionado:**
- `cleansing_lambda_name`
- `cleansing_lambda_arn`
- `cleansing_lambda_invoke_arn`
- `cleansing_lambda_version`
- Atualizado `s3_trigger_info` para incluir cleansing
- Atualizado `pipeline_summary` com cleansing Lambda
- Atualizado `lakehouse_summary` com Silver crawler features

---

## üóÇÔ∏è Particionamento Baseado em Eventos

### Compara√ß√£o: Ingest√£o vs Evento

#### Bronze (Particionado por Data de Ingest√£o)
```
s3://bronze-bucket/bronze/car_data/
‚îú‚îÄ‚îÄ ingest_year=2025/
‚îÇ   ‚îî‚îÄ‚îÄ ingest_month=10/
‚îÇ       ‚îî‚îÄ‚îÄ ingest_day=30/
‚îÇ           ‚îî‚îÄ‚îÄ file.parquet  (todos os dados ingeridos em 30/10)
```

**Uso:** Auditoria, rastreamento de quando os dados chegaram

#### Silver (Particionado por Data do Evento)
```
s3://silver-bucket/car_telemetry/
‚îú‚îÄ‚îÄ event_year=2025/
‚îÇ   ‚îî‚îÄ‚îÄ event_month=10/
‚îÇ       ‚îú‚îÄ‚îÄ event_day=28/  (eventos de 28/10, ingeridos em 30/10)
‚îÇ       ‚îú‚îÄ‚îÄ event_day=29/  (eventos de 29/10, ingeridos em 30/10)
‚îÇ       ‚îî‚îÄ‚îÄ event_day=30/  (eventos de 30/10, ingeridos em 30/10)
```

**Uso:** An√°lises temporais, trends, forecasting

### Exemplo de Query Eficiente (Athena)

**‚ùå SEM Particionamento:**
```sql
-- Scan completo de TODOS os arquivos (caro e lento)
SELECT COUNT(*) 
FROM silver_car_telemetry
WHERE metrics_metricTimestamp >= DATE '2025-10-28';
```
**Custo:** Scan de 100 GB = $0.50

**‚úÖ COM Particionamento:**
```sql
-- Scan apenas das parti√ß√µes necess√°rias
SELECT COUNT(*) 
FROM silver_car_telemetry
WHERE event_year = 2025 
  AND event_month = 10
  AND event_day >= 28;
```
**Custo:** Scan de 15 GB = $0.075  
**Economia:** 85% de custo! üéâ

---

## üöÄ Deployment e Configura√ß√£o

### Passo 1: Criar Pacote de Deployment

```powershell
# Navegar para a pasta da Lambda
cd c:\dev\HP\wsas\Poc\lambdas\cleansing

# Criar diret√≥rio tempor√°rio
New-Item -ItemType Directory -Force -Path .\package

# Instalar depend√™ncias (n√£o necess√°rio - usa Lambda Layer)
# pip install -r requirements.txt -t .\package

# Adicionar c√≥digo da Lambda
Copy-Item lambda_function.py .\package\

# Criar ZIP
Compress-Archive -Path .\package\* -DestinationPath ..\..\assets\cleansing_package.zip -Force

# Limpar diret√≥rio tempor√°rio
Remove-Item -Recurse -Force .\package

Write-Host "‚úÖ Pacote criado: assets/cleansing_package.zip"
```

### Passo 2: Aplicar Terraform

```powershell
cd c:\dev\HP\wsas\Poc\terraform

# Validar configura√ß√£o
terraform validate

# Ver plano de execu√ß√£o
terraform plan

# Aplicar mudan√ßas
terraform apply
```

**Recursos Criados:**
- ‚úÖ `aws_lambda_function.cleansing`
- ‚úÖ `aws_lambda_permission.allow_s3_invoke_cleansing`
- ‚úÖ `aws_s3_bucket_notification.bronze_bucket_notification`

**Recursos Modificados:**
- üîÑ `aws_glue_crawler.silver_crawler` (path e configura√ß√£o)

### Passo 3: Verificar Deployment

```powershell
# Verificar Lambda criada
aws lambda get-function --function-name datalake-pipeline-cleansing-dev --query "Configuration.[FunctionName,Runtime,MemorySize,Timeout]"

# Verificar trigger S3 configurado
aws lambda get-policy --function-name datalake-pipeline-cleansing-dev

# Verificar crawler atualizado
aws glue get-crawler --name datalake-pipeline-silver-crawler-dev --query "Crawler.[Name,Targets.S3Targets[0].Path,Configuration]"
```

---

## üß™ Testes e Valida√ß√£o

### Teste End-to-End

#### 1. **Upload de Arquivo JSON (Landing)**

```powershell
# Preparar arquivo de teste
$testJson = @"
{
  "carId": "TEST-001",
  "manufacturer": "tesla",
  "model": "Model 3",
  "year": 2024,
  "color": "Midnight Silver Metallic",
  "fuelCapacityLiters": 0,
  "metrics": {
    "engineTempCelsius": 22.5,
    "fuelAvailableLiters": 0,
    "speedKmh": 95.0,
    "metricTimestamp": "2025-10-30T14:30:45Z",
    "trip": {
      "tripMileage": 456.7,
      "tripFuelLiters": 0,
      "tripStartTimestamp": "2025-10-30T08:00:00Z"
    }
  },
  "carInsurance": {
    "provider": "Tesla Insurance",
    "policyNumber": "TSL-2024-789",
    "validUntil": "2026-12-31",
    "coverageType": "Full Coverage"
  },
  "market": {
    "marketRegion": "North America",
    "currentPrice": 45000.00,
    "currency": "USD"
  }
}
"@

# Salvar em arquivo
$testJson | Out-File -FilePath test_silver.json -Encoding UTF8

# Upload para Landing
aws s3 cp test_silver.json s3://datalake-pipeline-landing-dev/test_silver.json
```

#### 2. **Verificar Processamento Bronze**

```powershell
# Aguardar processamento (5-10 segundos)
Start-Sleep -Seconds 10

# Verificar logs da Ingestion Lambda
aws logs tail /aws/lambda/datalake-pipeline-ingestion-dev --follow --since 1m

# Verificar arquivo Parquet criado no Bronze
aws s3 ls s3://datalake-pipeline-bronze-dev/bronze/car_data/ingest_year=2025/ --recursive
```

**Sa√≠da Esperada:**
```
2025-10-30 14:31:02    1234 bronze/car_data/ingest_year=2025/ingest_month=10/ingest_day=30/test_silver_20251030_143102.parquet
```

#### 3. **Verificar Processamento Silver (Autom√°tico)**

```powershell
# Aguardar trigger e processamento (5-10 segundos)
Start-Sleep -Seconds 10

# Verificar logs da Cleansing Lambda
aws logs tail /aws/lambda/datalake-pipeline-cleansing-dev --follow --since 1m

# Verificar arquivo Parquet criado no Silver
aws s3 ls s3://datalake-pipeline-silver-dev/car_telemetry/event_year=2025/ --recursive
```

**Sa√≠da Esperada:**
```
2025-10-30 14:31:15    2345 car_telemetry/event_year=2025/event_month=10/event_day=30/test_silver_20251030_143102_20251030_143115.parquet
```

#### 4. **Executar Crawlers**

```powershell
# Bronze Crawler
aws glue start-crawler --name datalake-pipeline-bronze-car-data-crawler-dev

# Aguardar conclus√£o (30-60 segundos)
aws glue get-crawler --name datalake-pipeline-bronze-car-data-crawler-dev --query "Crawler.State"

# Silver Crawler
aws glue start-crawler --name datalake-pipeline-silver-crawler-dev

# Aguardar conclus√£o
aws glue get-crawler --name datalake-pipeline-silver-crawler-dev --query "Crawler.State"
```

#### 5. **Verificar Tabelas no Glue Catalog**

```powershell
# Listar tabelas
aws glue get-tables --database-name datalake-pipeline-catalog-dev --query "TableList[*].Name"

# Schema da tabela Bronze
aws glue get-table --database-name datalake-pipeline-catalog-dev --name bronze_ingest_year_2025 --query "Table.StorageDescriptor.Columns[*].[Name,Type]"

# Schema da tabela Silver
aws glue get-table --database-name datalake-pipeline-catalog-dev --name silver_car_telemetry --query "Table.StorageDescriptor.Columns[*].[Name,Type]"
```

**Schema Esperado (Silver):**
```
carId                              | string
manufacturer                       | string  (Title Case)
model                              | string
year                               | int
color                              | string  (lowercase)
fuelCapacityLiters                 | double
metrics_engineTempCelsius          | double
metrics_fuelAvailableLiters        | double
metrics_speedKmh                   | double
metrics_metricTimestamp            | timestamp
metrics_trip_tripMileage           | double
metrics_trip_tripFuelLiters        | double
metrics_trip_tripStartTimestamp    | timestamp
metrics_fuel_level_percentage      | double  (ENRIQUECIDO)
metrics_trip_km_per_liter          | double  (ENRIQUECIDO)
carInsurance_provider              | string
carInsurance_policyNumber          | string
carInsurance_validUntil            | date
carInsurance_coverageType          | string
market_marketRegion                | string
market_currentPrice                | double
market_currency                    | string
event_year                         | int     (PARTI√á√ÉO)
event_month                        | int     (PARTI√á√ÉO)
event_day                          | int     (PARTI√á√ÉO)
```

---

## üìä Queries Athena (Exemplos)

### 1. **Query B√°sica (Silver)**

```sql
-- Contar registros
SELECT COUNT(*) AS total_records
FROM silver_car_telemetry;
```

### 2. **An√°lise de M√©tricas Enriquecidas**

```sql
-- Top 10 ve√≠culos com menor efici√™ncia de combust√≠vel
SELECT 
    carId,
    manufacturer,
    model,
    AVG(metrics_trip_km_per_liter) AS avg_km_per_liter,
    AVG(metrics_fuel_level_percentage) AS avg_fuel_level_pct
FROM silver_car_telemetry
WHERE metrics_trip_km_per_liter IS NOT NULL
GROUP BY carId, manufacturer, model
ORDER BY avg_km_per_liter ASC
LIMIT 10;
```

### 3. **An√°lise Temporal (Com Parti√ß√µes)**

```sql
-- Velocidade m√©dia por dia (√∫ltima semana)
SELECT 
    event_year,
    event_month,
    event_day,
    AVG(metrics_speedKmh) AS avg_speed,
    MAX(metrics_speedKmh) AS max_speed,
    COUNT(*) AS num_readings
FROM silver_car_telemetry
WHERE event_year = 2025
  AND event_month = 10
  AND event_day >= 23  -- √öltimos 7 dias
GROUP BY event_year, event_month, event_day
ORDER BY event_year, event_month, event_day;
```

### 4. **An√°lise por Fabricante**

```sql
-- Estat√≠sticas por fabricante (ap√≥s limpeza Title Case)
SELECT 
    manufacturer,
    COUNT(DISTINCT carId) AS num_cars,
    AVG(metrics_engineTempCelsius) AS avg_temp,
    AVG(metrics_trip_km_per_liter) AS avg_efficiency,
    AVG(market_currentPrice) AS avg_price
FROM silver_car_telemetry
GROUP BY manufacturer
ORDER BY num_cars DESC;
```

### 5. **Join Bronze + Silver (Compara√ß√£o)**

```sql
-- Comparar estrutura aninhada (Bronze) vs achatada (Silver)
SELECT 
    b.carId,
    b.metrics.speedKmh AS bronze_speed,  -- Dot notation (Bronze)
    s.metrics_speedKmh AS silver_speed    -- Flat column (Silver)
FROM bronze_ingest_year_2025 b
JOIN silver_car_telemetry s
    ON b.carId = s.carId
LIMIT 10;
```

### 6. **An√°lise de Seguros (Data Conversion)**

```sql
-- Seguros expirando nos pr√≥ximos 90 dias
SELECT 
    carId,
    manufacturer,
    model,
    carInsurance_provider,
    carInsurance_validUntil,
    DATE_DIFF('day', CURRENT_DATE, CAST(carInsurance_validUntil AS DATE)) AS days_until_expiry
FROM silver_car_telemetry
WHERE CAST(carInsurance_validUntil AS DATE) <= CURRENT_DATE + INTERVAL '90' DAY
ORDER BY days_until_expiry ASC;
```

### 7. **An√°lise de Combust√≠vel (Enriquecimento)**

```sql
-- Ve√≠culos com n√≠vel cr√≠tico de combust√≠vel (<20%)
SELECT 
    carId,
    manufacturer,
    model,
    metrics_fuelAvailableLiters AS fuel_available,
    fuelCapacityLiters AS tank_capacity,
    metrics_fuel_level_percentage AS fuel_percentage,
    CASE 
        WHEN metrics_fuel_level_percentage < 10 THEN 'CR√çTICO'
        WHEN metrics_fuel_level_percentage < 20 THEN 'BAIXO'
        ELSE 'OK'
    END AS fuel_status
FROM silver_car_telemetry
WHERE metrics_fuel_level_percentage < 20
ORDER BY metrics_fuel_level_percentage ASC;
```

### 8. **Time Series Analysis (Parti√ß√µes Otimizadas)**

```sql
-- Tend√™ncia de velocidade m√©dia ao longo do tempo
SELECT 
    DATE(CAST(metrics_metricTimestamp AS TIMESTAMP)) AS event_date,
    AVG(metrics_speedKmh) AS avg_speed,
    STDDEV(metrics_speedKmh) AS speed_std_dev,
    COUNT(*) AS num_readings
FROM silver_car_telemetry
WHERE event_year = 2025
  AND event_month = 10
GROUP BY DATE(CAST(metrics_metricTimestamp AS TIMESTAMP))
ORDER BY event_date;
```

---

## üîç Troubleshooting

### Problema 1: Lambda N√£o √© Invocada

**Sintomas:**
- Arquivo Parquet criado no Bronze
- Nenhum arquivo criado no Silver
- Nenhuma entrada nos logs da Cleansing Lambda

**Diagn√≥stico:**
```powershell
# Verificar trigger configurado
aws s3api get-bucket-notification-configuration --bucket datalake-pipeline-bronze-dev

# Verificar permiss√£o Lambda
aws lambda get-policy --function-name datalake-pipeline-cleansing-dev
```

**Solu√ß√µes:**
1. ‚úÖ Verificar se `aws_s3_bucket_notification.bronze_bucket_notification` foi aplicado
2. ‚úÖ Verificar se `aws_lambda_permission.allow_s3_invoke_cleansing` existe
3. ‚úÖ Re-aplicar Terraform: `terraform apply`
4. ‚úÖ Testar upload manual: `aws s3 cp test.parquet s3://bronze-bucket/test.parquet`

---

### Problema 2: Erro "KeyError" ao Achatar Structs

**Sintomas:**
```
ERROR: KeyError: 'metrics'
Traceback: flatten_nested_columns()
```

**Causa:** Arquivo Bronze n√£o cont√©m as colunas esperadas (corrupto ou formato diferente)

**Diagn√≥stico:**
```powershell
# Baixar e inspecionar arquivo Bronze
aws s3 cp s3://bronze-bucket/bronze/car_data/.../file.parquet .
python -c "import pandas as pd; df = pd.read_parquet('file.parquet'); print(df.columns)"
```

**Solu√ß√£o:**
- Adicionar valida√ß√£o no c√≥digo:
```python
nested_cols = []
for col in df.columns:
    if len(df) > 0 and isinstance(df[col].iloc[0], dict):
        nested_cols.append(col)
```

---

### Problema 3: Divis√£o por Zero em `km_per_liter`

**Sintomas:**
```
WARNING: RuntimeWarning: divide by zero encountered in double_scalars
```

**Causa:** `metrics_trip_tripFuelLiters = 0`

**Solu√ß√£o Implementada:**
```python
df_enriched['metrics_trip_km_per_liter'] = df.apply(
    lambda row: (
        round(row['metrics_trip_tripMileage'] / 
              row['metrics_trip_tripFuelLiters'], 2)
        if row['metrics_trip_tripFuelLiters'] > 0
        else None  # ‚Üê Retorna NULL ao inv√©s de erro
    ),
    axis=1
)
```

---

### Problema 4: Crawler N√£o Detecta Parti√ß√µes

**Sintomas:**
- Tabela `silver_car_telemetry` criada
- Parti√ß√µes (`event_year`, `event_month`, `event_day`) n√£o aparecem como partition keys
- Queries n√£o filtram por parti√ß√µes

**Diagn√≥stico:**
```powershell
# Verificar estrutura S3
aws s3 ls s3://silver-bucket/car_telemetry/ --recursive

# Verificar configura√ß√£o do crawler
aws glue get-crawler --name datalake-pipeline-silver-crawler-dev --query "Crawler.Configuration"
```

**Solu√ß√£o:**
1. ‚úÖ Garantir estrutura HIVE: `event_year=2025/event_month=10/event_day=30/`
2. ‚úÖ Deletar tabela incorreta: `aws glue delete-table --database-name ... --name silver_car_telemetry`
3. ‚úÖ Re-executar crawler: `aws glue start-crawler --name ...`

---

### Problema 5: Timeout da Lambda (300s)

**Sintomas:**
```
ERROR: Task timed out after 300.00 seconds
```

**Causa:** Arquivo Parquet muito grande (>50 MB) ou processamento ineficiente

**Solu√ß√µes:**
1. ‚úÖ **Aumentar Timeout:** Editar `variables.tf` ‚Üí `timeout = 600` (10 min)
2. ‚úÖ **Aumentar Mem√≥ria:** `memory_size = 2048` (mais CPU dispon√≠vel)
3. ‚úÖ **Otimizar C√≥digo:** Processar em chunks
```python
# Processar em batches de 10000 linhas
chunk_size = 10000
for i in range(0, len(df), chunk_size):
    df_chunk = df.iloc[i:i+chunk_size]
    transform_and_write(df_chunk)
```

---

### Problema 6: Schema Drift (Novas Colunas)

**Sintomas:**
- Query retorna erro: "Column 'new_field' not found"
- Novos campos JSON n√£o aparecem em Athena

**Causa:** Crawler n√£o re-catalogou schema ap√≥s mudan√ßas

**Solu√ß√£o:**
```powershell
# Re-executar crawler (detecta novas colunas)
aws glue start-crawler --name datalake-pipeline-silver-crawler-dev

# Verificar configura√ß√£o "MergeNewColumns" no crawler
aws glue get-crawler --name datalake-pipeline-silver-crawler-dev \
    --query "Crawler.Configuration" --output json
```

**Garantir em Terraform:**
```hcl
configuration = jsonencode({
  CrawlerOutput = {
    Tables = {
      AddOrUpdateBehavior = "MergeNewColumns"  # ‚Üê CR√çTICO
    }
  }
})
```

---

## üéØ Pr√≥ximos Passos

### 1. **Pipeline Gold (Analytics Layer)**
- Criar agrega√ß√µes pr√©-computadas
- Views materializadas para dashboards
- Integra√ß√£o com QuickSight/Tableau

### 2. **Monitoramento e Alertas**
- CloudWatch Alarms para falhas de Lambda
- SNS notifications para erros
- Dashboards de m√©tricas (dura√ß√£o, custos)

### 3. **Data Quality Checks**
- Valida√ß√µes de schema (Great Expectations)
- Testes de integridade (duplicados, nulls)
- Alertas para anomalias

### 4. **Otimiza√ß√µes de Performance**
- Compacta√ß√£o de arquivos pequenos (S3 Small Files Problem)
- Particionamento adicional (por `manufacturer`, `region`)
- Uso de Z-Ordering para queries complexas

### 5. **CI/CD Pipeline**
- GitHub Actions para deploy autom√°tico
- Testes unit√°rios Python (pytest)
- Valida√ß√£o Terraform (terraform fmt, validate)

---

## üìö Refer√™ncias

### Documenta√ß√£o AWS
- [AWS Lambda Developer Guide](https://docs.aws.amazon.com/lambda/)
- [AWS Glue Crawler Configuration](https://docs.aws.amazon.com/glue/latest/dg/crawler-configuration.html)
- [Amazon Athena User Guide](https://docs.aws.amazon.com/athena/)
- [S3 Event Notifications](https://docs.aws.amazon.com/AmazonS3/latest/userguide/NotificationHowTo.html)

### Bibliotecas Python
- [Pandas Documentation](https://pandas.pydata.org/docs/)
- [PyArrow Parquet](https://arrow.apache.org/docs/python/parquet.html)
- [Boto3 (AWS SDK)](https://boto3.amazonaws.com/v1/documentation/api/latest/index.html)

### Terraform
- [Terraform AWS Provider](https://registry.terraform.io/providers/hashicorp/aws/latest/docs)
- [Lambda Resources](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/lambda_function)
- [S3 Notifications](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/s3_bucket_notification)

---

## ‚úÖ Checklist de Implementa√ß√£o

- [x] Lambda function criada (`cleansing-function`)
- [x] S3 trigger configurado (Bronze bucket ‚Üí `.parquet`)
- [x] Lambda Layer compartilhada (pandas + pyarrow)
- [x] L√≥gica de achatamento implementada (`json_normalize`)
- [x] Limpeza de dados (Title Case, lowercase)
- [x] Convers√£o de tipos (datetime, date)
- [x] Enriquecimento de m√©tricas (fuel %, km/L)
- [x] Particionamento por evento (event_year/month/day)
- [x] Glue Crawler atualizado (target `car_telemetry/`)
- [x] Terraform resources criados e testados
- [x] Outputs atualizados (Lambda ARNs, crawler info)
- [x] Documenta√ß√£o completa
- [x] Testes end-to-end validados
- [x] Queries Athena exemplificadas
- [x] Troubleshooting documentado

---

**Status Final:** ‚úÖ **Silver Layer IMPLEMENTADA E FUNCIONAL**

**Data:** 30 de Outubro de 2025  
**Autor:** Pipeline de Dados - AWS Lakehouse  
**Vers√£o:** 1.0.0
