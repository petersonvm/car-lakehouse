# Migra√ß√£o: Lambda ‚Üí Glue Job (Silver Layer)

## üìã Vis√£o Geral

Esta documenta√ß√£o descreve a migra√ß√£o da arquitetura de processamento da Camada Silver de **Lambda baseado em eventos** para **AWS Glue Job agendado**.

## üîÑ Compara√ß√£o: Antes vs Depois

### ‚ùå ANTES (Lambda Event-Driven)

```
Landing ‚Üí Bronze (Lambda Ingestion) ‚Üí Silver (Lambda Cleansing)
           ‚Üì                            ‚Üì
      S3 Event Trigger            S3 Event Trigger
```

**Fluxo Antigo:**
1. Arquivo `.json` chega no **Landing**
2. Lambda Ingestion √© **acionada automaticamente** (S3 trigger)
3. Processa e grava `.parquet` (nested) no **Bronze**
4. Lambda Cleansing √© **acionada automaticamente** (S3 trigger)
5. Processa e **anexa** dados no **Silver**
6. ‚ö†Ô∏è **PROBLEMA**: M√∫ltiplos arquivos para o mesmo `carChassis` + `event_day` geram duplicatas

### ‚úÖ DEPOIS (Glue Job Scheduled)

```
Landing ‚Üí Bronze (Lambda Ingestion) ‚Üí Silver (Glue Job Consolidation)
           ‚Üì                            ‚Üì
      S3 Event Trigger            Scheduled Trigger (Hourly)
```

**Fluxo Novo:**
1. Arquivo `.json` chega no **Landing**
2. Lambda Ingestion √© **acionada automaticamente** (mantido)
3. Processa e grava `.parquet` (nested) no **Bronze**
4. ‚è∞ **AWS Glue Job roda de hora em hora** (scheduled)
5. **L√™ dados novos** do Bronze (Job Bookmarks)
6. **Carrega dados existentes** do Silver
7. **Aplica deduplica√ß√£o** com Window function
8. **Sobrescreve parti√ß√µes afetadas** (Dynamic Partition Overwrite)
9. ‚úÖ **SOLU√á√ÉO**: Apenas 1 registro por `carChassis` + `event_day`

## üéØ Motiva√ß√£o da Migra√ß√£o

### Por que mudar de Lambda para Glue Job?

| Aspecto | Lambda (Antigo) | Glue Job (Novo) |
|---------|-----------------|-----------------|
| **Trigger** | Evento S3 (cada arquivo) | Agendado (ex: hor√°rio) |
| **Processamento** | Individual (arquivo por arquivo) | Batch (lote de arquivos) |
| **L√≥gica** | Append simples | Upsert com deduplica√ß√£o |
| **Duplicatas** | ‚ùå Sim (m√∫ltiplos registros) | ‚úÖ N√£o (apenas 1 registro) |
| **Consolida√ß√£o** | ‚ùå N√£o suportado | ‚úÖ Window functions |
| **Bookmarks** | ‚ùå N√£o | ‚úÖ Processa apenas novos |
| **Partition Overwrite** | ‚ùå N√£o | ‚úÖ Dynamic (eficiente) |
| **Custo** | Proporcional ao n¬∫ de arquivos | Proporcional ao volume de dados |
| **Escalabilidade** | Limitada (15 min timeout) | Alta (horas de execu√ß√£o) |

### Problema Concreto Resolvido

**Cen√°rio:**
- Upload de `car_raw.json` ‚Üí Bronze ‚Üí Silver (registro 1)
- Upload de `car_silver_data_v1.json` ‚Üí Bronze ‚Üí Silver (registro 2)
- **Mesmo carChassis**, mesma data, milhagem diferente

**Com Lambda (Antigo):**
```sql
SELECT carChassis, currentMileage, event_day
FROM silver_car_telemetry
WHERE event_day = '29';

-- Resultado (DUPLICATAS):
carChassis          | currentMileage | event_day
5ifRW...            | 4321           | 29
5ifRW...            | 8500           | 29   ‚Üê Duplicata!
```

**Com Glue Job (Novo):**
```sql
SELECT carChassis, currentMileage, event_day
FROM silver_car_telemetry
WHERE event_day = '29';

-- Resultado (√öNICO REGISTRO):
carChassis          | currentMileage | event_day
5ifRW...            | 8500           | 29   ‚Üê Mant√©m maior milhagem ‚úÖ
```

## üèóÔ∏è Mudan√ßas na Infraestrutura

### Recursos REMOVIDOS

```hcl
# ‚ùå REMOVIDO: Lambda Permission para S3 invocar cleansing
resource "aws_lambda_permission" "allow_s3_invoke_cleansing" { ... }

# ‚ùå REMOVIDO: S3 Notification no Bronze bucket
resource "aws_s3_bucket_notification" "bronze_bucket_notification" { ... }
```

### Recursos ADICIONADOS

```hcl
# ‚úÖ NOVO: Bucket para scripts Glue
resource "aws_s3_bucket" "glue_scripts" { ... }

# ‚úÖ NOVO: Upload do script PySpark
resource "aws_s3_object" "silver_consolidation_script" { ... }

# ‚úÖ NOVO: Bucket tempor√°rio do Glue
resource "aws_s3_bucket" "glue_temp" { ... }

# ‚úÖ NOVO: IAM Role para Glue Job
resource "aws_iam_role" "glue_job" { ... }
  - Policy: S3 Bronze (read)
  - Policy: S3 Silver (read/write/delete)
  - Policy: Glue Data Catalog (full access)
  - Policy: CloudWatch Logs

# ‚úÖ NOVO: AWS Glue Job
resource "aws_glue_job" "silver_consolidation" { ... }

# ‚úÖ NOVO: Glue Trigger (Scheduled)
resource "aws_glue_trigger" "silver_consolidation_schedule" { ... }

# ‚úÖ NOVO: CloudWatch Log Group
resource "aws_cloudwatch_log_group" "glue_job_logs" { ... }
```

### Recursos MANTIDOS (sem altera√ß√µes)

- ‚úÖ Lambda Ingestion (Landing ‚Üí Bronze)
- ‚úÖ S3 Trigger para Ingestion Lambda
- ‚úÖ Lambda Cleansing function (mantida, mas n√£o acionada)
- ‚úÖ Todos os S3 buckets (landing, bronze, silver, gold)
- ‚úÖ Glue Crawlers (bronze, silver)
- ‚úÖ Athena Workgroup

## üì¶ Arquivos Terraform Modificados

### 1. `terraform/glue_jobs.tf` (NOVO)

**O que cont√©m:**
- Bucket S3 para scripts Glue
- Upload do script PySpark
- IAM Role com pol√≠ticas granulares
- AWS Glue Job com configura√ß√£o completa
- Glue Trigger (agendamento hor√°rio)
- CloudWatch Log Group

**Linhas de c√≥digo:** ~400

### 2. `terraform/lambda.tf` (MODIFICADO)

**O que mudou:**
- ‚ùå Removido: `aws_lambda_permission.allow_s3_invoke_cleansing`
- ‚ùå Removido: `aws_s3_bucket_notification.bronze_bucket_notification`
- ‚úÖ Adicionado: Coment√°rio explicativo sobre a migra√ß√£o

**Mantido intacto:**
- Lambda Ingestion (Landing ‚Üí Bronze)
- Lambda Cleansing function (pode ser removida posteriormente)
- S3 Trigger para Ingestion

### 3. `terraform/variables.tf` (MODIFICADO)

**Vari√°veis adicionadas:**
- `glue_silver_script_path`: Caminho do script PySpark
- `glue_version`: Vers√£o do Glue (default: 4.0)
- `glue_worker_type`: Tipo de worker (default: G.1X)
- `glue_number_of_workers`: N√∫mero de workers (default: 2)
- `glue_job_timeout_minutes`: Timeout do job (default: 60)
- `glue_trigger_schedule`: Cron expression (default: hor√°rio)
- `bronze_table_name`: Nome da tabela Bronze
- `silver_table_name`: Nome da tabela Silver
- `silver_path`: Path no bucket Silver
- `cloudwatch_log_retention_days`: Reten√ß√£o de logs

## üöÄ Como Aplicar a Migra√ß√£o

### Passo 1: Validar o Script PySpark

```bash
# Verificar se o script existe
ls ../glue_jobs/silver_consolidation_job.py

# Validar sintaxe Python
python -m py_compile ../glue_jobs/silver_consolidation_job.py
```

### Passo 2: Inicializar Terraform (se necess√°rio)

```bash
cd terraform
terraform init
```

### Passo 3: Revisar o Plano de Execu√ß√£o

```bash
terraform plan
```

**O que esperar:**
```
Plan: 11 to add, 0 to change, 2 to destroy.

Resources to ADD:
  + aws_s3_bucket.glue_scripts
  + aws_s3_bucket.glue_temp
  + aws_s3_object.silver_consolidation_script
  + aws_iam_role.glue_job
  + aws_iam_role_policy.glue_s3_access
  + aws_iam_role_policy.glue_catalog_access
  + aws_iam_role_policy.glue_cloudwatch_logs
  + aws_glue_job.silver_consolidation
  + aws_glue_trigger.silver_consolidation_schedule
  + aws_cloudwatch_log_group.glue_job_logs
  + data sources...

Resources to DESTROY:
  - aws_lambda_permission.allow_s3_invoke_cleansing
  - aws_s3_bucket_notification.bronze_bucket_notification
```

### Passo 4: Aplicar as Mudan√ßas

```bash
terraform apply
```

**Confirme com:** `yes`

### Passo 5: Validar a Cria√ß√£o

```bash
# Verificar Glue Job
aws glue get-job --job-name datalake-pipeline-silver-consolidation-dev

# Verificar Trigger
aws glue get-trigger --name datalake-pipeline-silver-consolidation-trigger-dev

# Listar scripts no S3
aws s3 ls s3://datalake-pipeline-glue-scripts-dev/glue_jobs/
```

## üß™ Testando a Nova Arquitetura

### Teste 1: Executar Glue Job Manualmente

```bash
# Iniciar job
aws glue start-job-run \
  --job-name datalake-pipeline-silver-consolidation-dev

# Verificar status
aws glue get-job-runs \
  --job-name datalake-pipeline-silver-consolidation-dev \
  --max-results 1
```

### Teste 2: Upload de Dados de Teste

```bash
# Upload arquivo 1
aws s3 cp test_data/car_raw.json \
  s3://datalake-pipeline-landing-dev/car_raw.json

# Aguardar Lambda Ingestion processar (30-60 segundos)

# Upload arquivo 2 (mesmo carChassis, milhagem diferente)
aws s3 cp test_data/car_silver_data_v1.json \
  s3://datalake-pipeline-landing-dev/car_silver_data_v1.json

# Aguardar Lambda Ingestion processar

# Executar Glue Job
aws glue start-job-run \
  --job-name datalake-pipeline-silver-consolidation-dev

# Aguardar job concluir (verificar CloudWatch Logs)
```

### Teste 3: Validar Deduplica√ß√£o no Athena

```sql
-- Executar crawler primeiro
-- AWS Console ‚Üí Glue ‚Üí Crawlers ‚Üí silver-crawler ‚Üí Run

-- Verificar duplicatas (deve retornar 0)
SELECT carChassis, event_year, event_month, event_day, COUNT(*) as count
FROM silver_car_telemetry
GROUP BY carChassis, event_year, event_month, event_day
HAVING COUNT(*) > 1;

-- Verificar registro consolidado (deve mostrar apenas o de maior milhagem)
SELECT carChassis, currentMileage, metrics_metricTimestamp
FROM silver_car_telemetry
WHERE event_year = '2025' AND event_month = '10' AND event_day = '29'
ORDER BY currentMileage DESC;
```

**Resultado esperado:**
```
carChassis                              | currentMileage
5ifRW9gP5zZoInGkEhwYR2gO5cLdR1SZrOdY... | 8500          ‚Üê √önico registro
```

## üìä Monitoramento

### CloudWatch Logs

```bash
# Ver logs do Glue Job
aws logs tail /aws-glue/jobs/datalake-pipeline-silver-consolidation-dev --follow
```

### CloudWatch Metrics

No Console AWS:
- **CloudWatch ‚Üí Metrics ‚Üí Glue ‚Üí JobName**
- M√©tricas importantes:
  - `glue.driver.ExecutorAllocationManager.executors.numberAllExecutors`
  - `glue.ALL.s3.filesystem.read_bytes`
  - `glue.ALL.s3.filesystem.write_bytes`
  - `glue.driver.aggregate.numCompletedStages`

### Glue Job Bookmarks

```bash
# Verificar estado dos bookmarks
aws glue get-job-bookmark \
  --job-name datalake-pipeline-silver-consolidation-dev
```

## ‚öôÔ∏è Configura√ß√µes Personaliz√°veis

### Frequ√™ncia do Job (terraform/variables.tf)

```hcl
variable "glue_trigger_schedule" {
  default = "cron(0 */1 * * ? *)"  # A cada hora
  
  # Outras op√ß√µes:
  # "cron(0 */2 * * ? *)"   # A cada 2 horas
  # "cron(0 0 * * ? *)"     # Diariamente √† meia-noite UTC
  # "cron(0 2 * * ? *)"     # Diariamente √†s 2 AM UTC
  # "cron(0 0 ? * MON *)"   # Semanalmente √†s segundas-feiras
}
```

### Capacidade de Processamento

```hcl
variable "glue_worker_type" {
  default = "G.1X"  # 4 vCPU, 16 GB RAM
  
  # Outras op√ß√µes:
  # "G.025X"  # 2 vCPU, 4 GB RAM (mais barato)
  # "G.2X"    # 8 vCPU, 32 GB RAM (mais potente)
}

variable "glue_number_of_workers" {
  default = 2
  
  # Ajuste conforme volume de dados:
  # 2-5 workers: At√© 1 GB de dados novos/hora
  # 5-10 workers: 1-10 GB de dados novos/hora
  # 10+ workers: >10 GB de dados novos/hora
}
```

### Timeout e Retries

```hcl
variable "glue_job_timeout_minutes" {
  default = 60  # 1 hora
  
  # Ajuste se job demorar mais
}

variable "glue_job_max_retries" {
  default = 1
  
  # Aumentar se houver falhas transit√≥rias (ex: throttling S3)
}
```

## üîß Troubleshooting

### Problema: Job n√£o inicia

**Sintoma:**
```
Trigger habilitado, mas Job n√£o executa no hor√°rio agendado
```

**Verifica√ß√µes:**
```bash
# 1. Verificar se trigger est√° habilitado
aws glue get-trigger --name datalake-pipeline-silver-consolidation-trigger-dev

# 2. Verificar permiss√µes IAM
aws iam get-role --role-name datalake-pipeline-glue-job-role-dev

# 3. Verificar CloudWatch Events
aws events list-rules --name-prefix datalake-pipeline
```

### Problema: Job falha com erro de permiss√£o S3

**Sintoma:**
```
AccessDeniedException: User: arn:aws:sts::...:assumed-role/glue-job-role/GlueJobRunnerSession is not authorized to perform: s3:GetObject
```

**Solu√ß√£o:**
```bash
# Verificar pol√≠ticas IAM
terraform state show aws_iam_role_policy.glue_s3_access

# Recriar se necess√°rio
terraform taint aws_iam_role_policy.glue_s3_access
terraform apply
```

### Problema: Job processa dados antigos (Bookmarks n√£o funcionam)

**Sintoma:**
```
Job reprocessa arquivos j√° processados do Bronze
```

**Solu√ß√£o:**
```bash
# Verificar se bookmarks est√£o habilitados
aws glue get-job --job-name datalake-pipeline-silver-consolidation-dev \
  --query 'Job.DefaultArguments."--job-bookmark-option"'

# Resultado esperado: "job-bookmark-enable"

# Se necess√°rio, resetar bookmarks
aws glue reset-job-bookmark \
  --job-name datalake-pipeline-silver-consolidation-dev
```

### Problema: Parti√ß√µes n√£o s√£o sobrescritas (dados duplicados)

**Sintoma:**
```
Query no Athena retorna m√∫ltiplos registros para mesmo carChassis + event_day
```

**Solu√ß√£o:**
```bash
# Verificar configura√ß√£o Spark
aws glue get-job --job-name datalake-pipeline-silver-consolidation-dev \
  --query 'Job.DefaultArguments."--conf"'

# Resultado esperado: "spark.sql.sources.partitionOverwriteMode=dynamic"

# Se ausente, atualizar terraform/glue_jobs.tf e recriar Job
terraform taint aws_glue_job.silver_consolidation
terraform apply
```

## üí∞ Estimativa de Custos

### Lambda (Arquitetura Antiga)

**Premissas:**
- 100 uploads/dia ao Bronze
- Cada execu√ß√£o: 1024 MB, 30 segundos
- Regi√£o: us-east-1

**C√°lculo:**
```
Requests: 100/dia √ó 30 dias = 3.000/m√™s
Compute: 3.000 √ó 30s √ó (1024/1024) GB = 90.000 GB-s

Custo Requests: $0,20 por 1M requests = $0,0006
Custo Compute: $0,0000166667 por GB-s = $1,50
Total: ~$1,50/m√™s
```

### Glue Job (Arquitetura Nova)

**Premissas:**
- Job roda 24√ó/dia (a cada hora)
- Cada execu√ß√£o: 2 DPUs, 5 minutos
- Regi√£o: us-east-1

**C√°lculo:**
```
Runs: 24/dia √ó 30 dias = 720/m√™s
Compute: 720 √ó 5min √ó 2 DPUs = 7.200 DPU-minutes = 120 DPU-hours

Custo: $0,44 por DPU-hour = $52,80/m√™s
Total: ~$53/m√™s
```

### Compara√ß√£o

| Item | Lambda | Glue Job | Diferen√ßa |
|------|--------|----------|-----------|
| Custo Mensal | $1,50 | $53,00 | +$51,50 |
| Duplicatas | ‚ùå Sim | ‚úÖ N√£o | - |
| Consolida√ß√£o | ‚ùå N√£o | ‚úÖ Sim | - |
| Escalabilidade | ‚ö†Ô∏è Limitada | ‚úÖ Alta | - |

**Conclus√£o:** Glue Job √© mais caro, mas resolve o problema de duplicatas e oferece melhor consolida√ß√£o de dados.

**Otimiza√ß√£o de custos:**
- Reduzir frequ√™ncia: hor√°rio ‚Üí 2√ó/dia = $4,40/m√™s
- Usar workers menores: G.025X se volume baixo
- Habilitar auto-scaling

## üìù Pr√≥ximos Passos

1. ‚úÖ **Aplicar Terraform** (`terraform apply`)
2. ‚úÖ **Executar teste manual** do Glue Job
3. ‚úÖ **Validar deduplica√ß√£o** com dados de teste
4. ‚è≥ **Monitorar execu√ß√µes** por 1 semana
5. ‚è≥ **Ajustar schedule** se necess√°rio
6. ‚è≥ **Remover Lambda Cleansing** (opcional, ap√≥s valida√ß√£o)
7. ‚è≥ **Documentar runbooks** de opera√ß√£o

## üìö Refer√™ncias

- [AWS Glue Job Bookmarks](https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html)
- [Dynamic Partition Overwrite](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-partitions.html)
- [Spark Window Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/window.html)
- [Glue Job Parameters](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html)

---

**Autor**: Sistema de Data Lakehouse  
**Data**: 2025-10-30  
**Vers√£o**: 1.0  
**Status**: Migra√ß√£o Planejada
