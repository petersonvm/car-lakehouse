"""
AWS Glue ETL Job - Gold Layer: Car Current State
=================================================

Objetivo:
- Ler dados consolidados da Camada Silver (hist√≥rico completo)
- Aplicar l√≥gica de "Estado Atual" (1 linha por carChassis)
- Escrever snapshot no Gold Bucket (sobrescrita total)

L√≥gica de Neg√≥cio:
- Chave de neg√≥cio: carChassis
- Regra de sele√ß√£o: currentMileage DESC (o maior = mais recente)
- Resultado: 1 registro √∫nico por ve√≠culo (estado atual)

Caracter√≠sticas:
- Leitura: Tabela Silver completa (sem particionamento)
- Transforma√ß√£o: Window Function (row_number)
- Escrita: Overwrite completo (snapshot est√°tico)
- Sa√≠da: Parquet n√£o-particionado (tabela pequena)

Autor: Sistema de Data Lakehouse - Camada Gold
Data: 2025-10-30
"""

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

from pyspark.sql import functions as F
from pyspark.sql.window import Window
from datetime import datetime

# ============================================================================
# 1. INICIALIZA√á√ÉO DO JOB
# ============================================================================

print("\n" + "=" * 80)
print("ü•á AWS GLUE JOB - GOLD LAYER: CAR CURRENT STATE")
print("=" * 80)

# Obter par√¢metros do Job
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'silver_database',
    'silver_table',
    'gold_database',
    'gold_bucket',
    'gold_path'
])

print(f"\nüìã Par√¢metros do Job:")
print(f"   Job Name: {args['JOB_NAME']}")
print(f"   Silver Database: {args['silver_database']}")
print(f"   Silver Table: {args['silver_table']}")
print(f"   Gold Database: {args['gold_database']}")
print(f"   Gold Bucket: {args['gold_bucket']}")
print(f"   Gold Path: {args['gold_path']}")

# Inicializar contextos Spark e Glue
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

print("\n‚úÖ Contextos Spark e Glue inicializados com sucesso")

# ============================================================================
# 2. LEITURA DA CAMADA SILVER
# ============================================================================

print("\n" + "=" * 80)
print("üìö ETAPA 1: Lendo dados consolidados da Camada Silver")
print("=" * 80)

print(f"\n   Database: {args['silver_database']}")
print(f"   Table: {args['silver_table']}")

# Ler tabela Silver completa do Glue Data Catalog
silver_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(
    database=args['silver_database'],
    table_name=args['silver_table'],
    transformation_ctx="silver_data"
)

# Converter para DataFrame
df_silver = silver_dynamic_frame.toDF()

# Contar registros totais
total_records = df_silver.count()
print(f"\n   ‚úÖ Registros lidos da Silver: {total_records}")

if total_records == 0:
    print("\n   ‚ö†Ô∏è  AVISO: Nenhum dado encontrado na Camada Silver!")
    print("   Finalizando job sem gerar dados no Gold.")
    job.commit()
    print("\n‚úÖ JOB CONCLU√çDO (sem dados para processar)")
    # N√£o usar sys.exit() - deixar completar naturalmente
else:
    print("\n   üìä Schema da Camada Silver:")
    df_silver.printSchema()

    # Mostrar amostra dos dados Silver
    print("\n   üìã Amostra de dados Silver (primeiros 5 registros):")
    df_silver.select(
        "carChassis",
        "currentMileage",
        "metrics_metricTimestamp",
        "event_year",
        "event_month",
        "event_day"
    ).show(5, truncate=False)

    # ============================================================================
    # 3. TRANSFORMA√á√ÉO: ESTADO ATUAL (WINDOW FUNCTION)
    # ============================================================================

    print("\n" + "=" * 80)
    print("üîÑ ETAPA 2: Aplicando l√≥gica de 'Estado Atual'")
    print("=" * 80)

    print("\n   üéØ Regra de Neg√≥cio:")
    print("      - 1 registro por carChassis (ve√≠culo)")
    print("      - Crit√©rio: MAIOR currentMileage (mais recente)")
    print("      - M√©todo: Window Function com row_number()")

    # Definir Window Specification
    # Particionar por: carChassis (cada ve√≠culo)
    # Ordenar por: currentMileage DESC (maior milhagem = mais recente)
    window_spec = Window.partitionBy("carChassis").orderBy(F.col("currentMileage").desc())

    print("\n   üîπ Aplicando Window Function...")

    # Adicionar coluna row_number
    df_with_row_number = df_silver.withColumn(
        "row_num",
        F.row_number().over(window_spec)
    )

    print("      ‚úÖ row_number() aplicado")

    # Filtrar apenas row_number = 1 (estado atual)
    df_current_state = df_with_row_number.filter(F.col("row_num") == 1).drop("row_num")

    current_state_count = df_current_state.count()
    vehicles_deduped = total_records - current_state_count

    print(f"\n   üìä Resultado da Transforma√ß√£o:")
    print(f"      - Registros hist√≥ricos (Silver): {total_records}")
    print(f"      - Registros de estado atual (Gold): {current_state_count}")
    print(f"      - Registros hist√≥ricos descartados: {vehicles_deduped}")
    print(f"      - Taxa de redu√ß√£o: {(vehicles_deduped / total_records * 100):.1f}%")

    # ============================================================================
    # 4. ENRIQUECIMENTO ADICIONAL (OPCIONAL)
    # ============================================================================

    print("\n" + "=" * 80)
    print("üîß ETAPA 3: Enriquecimento adicional da Camada Gold")
    print("=" * 80)

    # Adicionar timestamp de processamento (metadado Gold)
    df_gold = df_current_state.withColumn(
        "gold_processing_timestamp",
        F.current_timestamp()
    ).withColumn(
        "gold_snapshot_date",
        F.current_date()
    )

    print("   ‚úÖ Metadados Gold adicionados:")
    print("      - gold_processing_timestamp: timestamp da execu√ß√£o do job")
    print("      - gold_snapshot_date: data do snapshot")

    # Calcular m√©tricas agregadas (opcional - exemplo)
    print("\n   üìä Estat√≠sticas do Estado Atual:")
    
    # Contar ve√≠culos por fabricante
    manufacturer_stats = df_gold.groupBy("Manufacturer").count().orderBy(F.col("count").desc())
    print("\n   üè≠ Ve√≠culos por Fabricante:")
    manufacturer_stats.show(10, truncate=False)

    # Mostrar amostra dos dados Gold finais
    print("\n   üìã Amostra de dados Gold (estado atual - primeiros 5 ve√≠culos):")
    df_gold.select(
        "carChassis",
        "Manufacturer",
        "Model",
        "currentMileage",
        "metrics_metricTimestamp",
        "gold_processing_timestamp"
    ).orderBy(F.col("currentMileage").desc()).show(5, truncate=False)

    # ============================================================================
    # 5. ESCRITA NO GOLD BUCKET (OVERWRITE COMPLETO)
    # ============================================================================

    print("\n" + "=" * 80)
    print("üíæ ETAPA 4: Escrevendo dados no Gold Bucket")
    print("=" * 80)

    gold_output_path = f"s3://{args['gold_bucket']}/{args['gold_path']}"
    
    print(f"\n   üì¶ Configura√ß√£o de Escrita:")
    print(f"      - Bucket: {args['gold_bucket']}")
    print(f"      - Path: {args['gold_path']}")
    print(f"      - Full Path: {gold_output_path}")
    print(f"      - Modo: overwrite (snapshot completo)")
    print(f"      - Formato: parquet")
    print(f"      - Compress√£o: snappy")
    print(f"      - Particionamento: Nenhum (tabela pequena)")

    print("\n   üöÄ Iniciando escrita...")

    # Escrever dados usando Spark DataFrame Writer
    # Modo overwrite: sobrescreve todo o diret√≥rio (snapshot est√°tico)
    df_gold.write \
        .mode("overwrite") \
        .format("parquet") \
        .option("compression", "snappy") \
        .save(gold_output_path)

    print(f"\n   ‚úÖ Dados escritos com sucesso!")
    print(f"   üìä Total de registros no Gold: {current_state_count}")

    # Verificar arquivos escritos
    print("\n   üìÇ Arquivos Parquet gerados:")
    try:
        files_df = spark.read.parquet(gold_output_path)
        num_files = len([f for f in spark._jvm.org.apache.hadoop.fs.FileSystem.get(
            spark._jsc.hadoopConfiguration()
        ).listStatus(
            spark._jvm.org.apache.hadoop.fs.Path(gold_output_path)
        ) if f.getPath().getName().endswith(".parquet")])
        print(f"      - N√∫mero de arquivos: {num_files}")
        print(f"      - Total de registros: {files_df.count()}")
    except Exception as e:
        print(f"      ‚ö†Ô∏è  N√£o foi poss√≠vel listar arquivos: {e}")

    # ============================================================================
    # 6. FINALIZA√á√ÉO DO JOB
    # ============================================================================

    print("\n" + "=" * 80)
    print("‚úÖ JOB CONCLU√çDO COM SUCESSO!")
    print("=" * 80)
    
    print(f"\nüìä Resumo Final:")
    print(f"   - Registros lidos (Silver): {total_records}")
    print(f"   - Ve√≠culos √∫nicos (Gold): {current_state_count}")
    print(f"   - Redu√ß√£o de dados: {(vehicles_deduped / total_records * 100):.1f}%")
    print(f"   - Output Path: {gold_output_path}")
    print(f"   - Snapshot Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    print("\n" + "=" * 80)
    print("üéØ Pr√≥ximos Passos:")
    print("   1. Workflow ir√° acionar Gold Crawler automaticamente")
    print("   2. Crawler atualizar√° tabela 'gold_car_current_state' no cat√°logo")
    print("   3. Dados estar√£o dispon√≠veis para consulta no Athena")
    print("=" * 80)

    # Commit do Job
    job.commit()

    print("\n‚úÖ Job commit realizado com sucesso")
