"""
AWS Glue ETL Job - Silver Layer Consolidation (REFATORADO)
===========================================================

Objetivo:
- Ler dados novos do Bronze (Parquet com m√∫ltiplas estruturas aninhadas)
- Aplicar achatamento (flatten) de structs complexos
- Consolidar estado atual por ve√≠culo (current state)
- Enriquecer com KPIs de seguro
- Escrever apenas parti√ß√µes afetadas (Dynamic Partition Overwrite)

Nova Estrutura Bronze:
- M√∫ltiplos timestamps de extra√ß√£o
- Estruturas aninhadas (vehicle_static_info, vehicle_dynamic_state, trip_data)
- Dados distribu√≠dos em data.* de cada struct

L√≥gica de Consolida√ß√£o:
- Chave de neg√≥cio: carChassis 
- Regra de preced√™ncia: current_mileage DESC (estado mais atual)
- Resultado: 1 registro √∫nico por carChassis (estado atual)

Autor: Sistema de Data Lakehouse
Data: 2025-11-04 (Refatora√ß√£o)
"""

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from awsglue.dynamicframe import DynamicFrame

from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.sql.types import TimestampType, DateType, DoubleType
from datetime import datetime

# ============================================================================
# 1. INICIALIZA√á√ÉO DO JOB
# ============================================================================

# Obter par√¢metros do Job
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'bronze_database',
    'bronze_table',
    'silver_database',
    'silver_table',
    'silver_bucket',
    'silver_path'
])

# Inicializar contextos Spark e Glue
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# Configurar Spark para Dynamic Partition Overwrite
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

# Configurar Spark para usar parser de datas LEGACY (compat√≠vel com Spark 2.x)
# Necess√°rio para reconhecer formato 'EEE, dd MMM yyyy HH:mm:ss'
spark.conf.set("spark.sql.legacy.timeParserPolicy", "LEGACY")

print("=" * 80)
print(f"üöÄ Job iniciado: {args['JOB_NAME']}")
print(f"üìÖ Timestamp: {datetime.now().isoformat()}")
print("=" * 80)

# ============================================================================
# 2. LEITURA DOS DADOS NOVOS DO BRONZE (ESTRUTURA ANINHADA)
# ============================================================================

print("\nüì• ETAPA 1: Leitura de dados novos do Bronze (Estrutura Aninhada)...")
print(f"   Database: {args['bronze_database']}")
print(f"   Table: {args['bronze_table']}")

# Ler dados do Bronze usando Glue Data Catalog com Bookmarks
# transformation_ctx √© CRUCIAL para rastrear o que j√° foi processado
bronze_dynamic_frame = glueContext.create_dynamic_frame.from_catalog(
    database=args['bronze_database'],
    table_name=args['bronze_table'],
    transformation_ctx="bronze_source"  # Bookmark tracking
)

# Converter para Spark DataFrame
df_bronze_nested = bronze_dynamic_frame.toDF()

# Contar registros novos
new_records_count = df_bronze_nested.count()
print(f"   ‚úÖ Registros novos encontrados: {new_records_count}")

# Mostrar schema do Bronze (aninhado complexo)
print("\n   üìä Schema Bronze (com m√∫ltiplos structs aninhados):")
df_bronze_nested.printSchema()

if new_records_count == 0:
    print("   ‚ÑπÔ∏è  Nenhum registro novo para processar. Job continuar√° para manter bookmarks atualizados.")
else:
    print(f"   üîç Exemplo de dados Bronze aninhados:")
    df_bronze_nested.select("event_id", "carChassis", "event_primary_timestamp").show(2, truncate=False)

# ============================================================================
# 3. ACHATAMENTO (FLATTENING) DA ESTRUTURA ANINHADA COMPLEXA
# ============================================================================

print("\nüîÑ ETAPA 2: Achatamento de estruturas aninhadas complexas...")

if new_records_count > 0:
    
    # ----------------------------------------------------------------------------
    # 3.1 ACHATAMENTO PRINCIPAL - Extrair dados dos structs aninhados
    # ----------------------------------------------------------------------------
    
    print("   üîπ 1/4: Achatando m√∫ltiplos structs aninhados...")
    
    # Achatamento completo: extrair campos 'data' de cada struct
    df_flattened = df_bronze_nested.select(
        # Campos principais do evento
        F.col("event_id"),
        F.col("carChassis"),
        F.to_timestamp(F.col("event_primary_timestamp")).alias("event_timestamp"),
        F.col("processing_timestamp"),
        
        # Vehicle Static Info - achatar data.*
        F.col("vehicle_static_info.data.Model").alias("vehicle_model"),
        F.col("vehicle_static_info.data.year").alias("vehicle_year"),
        F.col("vehicle_static_info.data.ModelYear").alias("vehicle_model_year"),
        F.col("vehicle_static_info.data.Manufacturer").alias("vehicle_manufacturer"),
        F.col("vehicle_static_info.data.gasType").alias("vehicle_gas_type"),
        F.col("vehicle_static_info.data.fuelCapacityLiters").alias("vehicle_fuel_capacity_liters"),
        F.col("vehicle_static_info.data.color").alias("vehicle_color"),
        
        # Vehicle Dynamic State - Insurance Info
        F.col("vehicle_dynamic_state.insurance_info.data.provider").alias("insurance_provider"),
        F.col("vehicle_dynamic_state.insurance_info.data.policy_number").alias("insurance_policy_number"),
        F.to_date(F.col("vehicle_dynamic_state.insurance_info.data.validUntil"), "yyyy-MM-dd").alias("insurance_valid_until"),
        
        # Vehicle Dynamic State - Maintenance Info
        F.to_date(F.col("vehicle_dynamic_state.maintenance_info.data.last_service_date"), "yyyy-MM-dd").alias("maintenance_last_service_date"),
        F.col("vehicle_dynamic_state.maintenance_info.data.last_service_mileage").alias("maintenance_last_service_mileage"),
        F.col("vehicle_dynamic_state.maintenance_info.data.oil_life_percentage").alias("maintenance_oil_life_percentage"),
        
        # Current Rental Agreement
        F.col("current_rental_agreement.data.agreement_id").alias("rental_agreement_id"),
        F.col("current_rental_agreement.data.customer_id").alias("rental_customer_id"),
        F.to_timestamp(F.col("current_rental_agreement.data.rental_start_date")).alias("rental_start_date"),
        
        # Trip Data - Trip Summary
        F.to_timestamp(F.col("trip_data.trip_summary.data.tripStartTimestamp")).alias("trip_start_timestamp"),
        F.to_timestamp(F.col("trip_data.trip_summary.data.tripEndTimestamp")).alias("trip_end_timestamp"),
        F.col("trip_data.trip_summary.data.tripMileage").alias("trip_mileage"),
        F.col("trip_data.trip_summary.data.tripTimeMinutes").alias("trip_time_minutes"),
        F.col("trip_data.trip_summary.data.tripFuelLiters").alias("trip_fuel_liters"),
        F.col("trip_data.trip_summary.data.tripMaxSpeedKm").alias("trip_max_speed_km"),
        
        # Trip Data - Vehicle Telemetry Snapshot
        F.col("trip_data.vehicle_telemetry_snapshot.data.currentMileage").alias("current_mileage"),
        F.col("trip_data.vehicle_telemetry_snapshot.data.fuelAvailableLiters").alias("fuel_available_liters"),
        F.col("trip_data.vehicle_telemetry_snapshot.data.engineTempCelsius").alias("engine_temp_celsius"),
        F.col("trip_data.vehicle_telemetry_snapshot.data.oilTempCelsius").alias("oil_temp_celsius"),
        F.col("trip_data.vehicle_telemetry_snapshot.data.batteryChargePerc").alias("battery_charge_percentage"),
        F.col("trip_data.vehicle_telemetry_snapshot.data.tire_pressures_psi.front_left").alias("tire_pressure_front_left"),
        F.col("trip_data.vehicle_telemetry_snapshot.data.tire_pressures_psi.front_right").alias("tire_pressure_front_right"),
        F.col("trip_data.vehicle_telemetry_snapshot.data.tire_pressures_psi.rear_left").alias("tire_pressure_rear_left"),
        F.col("trip_data.vehicle_telemetry_snapshot.data.tire_pressures_psi.rear_right").alias("tire_pressure_rear_right"),
        
        # Parti√ß√µes originais (mantemos para compatibilidade)
        F.col("ingest_year"),
        F.col("ingest_month"),
        F.col("ingest_day")
    )
    
    print(f"      ‚úÖ {len(df_flattened.columns)} colunas ap√≥s achatamento")
    
    # ----------------------------------------------------------------------------
    # 3.2 LIMPEZA E PADRONIZA√á√ÉO
    # ----------------------------------------------------------------------------
    
    print("   üîπ 2/4: Aplicando limpeza e padroniza√ß√£o...")
    
    df_clean = df_flattened.withColumn(
        "vehicle_manufacturer",
        F.initcap(F.col("vehicle_manufacturer"))  # Title Case
    ).withColumn(
        "vehicle_color",
        F.lower(F.col("vehicle_color"))  # lowercase
    ).withColumn(
        "insurance_provider",
        F.initcap(F.col("insurance_provider"))  # Title Case
    )
    
    print("      ‚úÖ Padroniza√ß√£o aplicada: manufacturer/provider ‚Üí Title Case, color ‚Üí lowercase")
    
    # ----------------------------------------------------------------------------
    # 3.3 ENRIQUECIMENTO - KPIs Calculados
    # ----------------------------------------------------------------------------
    
    print("   üîπ 3/4: Calculando KPIs enriquecidos...")
    
    df_enriched = df_clean.withColumn(
        "fuel_level_percentage",
        F.round((F.col("fuel_available_liters") / F.col("vehicle_fuel_capacity_liters")) * 100, 2)
    ).withColumn(
        "trip_avg_speed_km",
        F.when(
            F.col("trip_time_minutes") > 0,
            F.round((F.col("trip_mileage") / F.col("trip_time_minutes")) * 60, 2)
        ).otherwise(0.0)
    ).withColumn(
        "trip_fuel_efficiency_km_per_liter",
        F.when(
            F.col("trip_fuel_liters") > 0,
            F.round(F.col("trip_mileage") / F.col("trip_fuel_liters"), 2)
        ).otherwise(0.0)
    )
    
    print("      ‚úÖ KPIs calculados: fuel_level_percentage, trip_avg_speed_km, fuel_efficiency")
    
    # ----------------------------------------------------------------------------
    # 3.4 KPIs DE SEGURO (INSURANCE KPIs) - MANTIDOS DA VERS√ÉO ANTERIOR
    # ----------------------------------------------------------------------------
    
    print("   üîπ 4/4: Calculando KPIs de Seguro...")
    
    # Calcular dias at√© vencimento do seguro
    df_with_insurance_kpis = df_enriched.withColumn(
        "insurance_days_to_expiry",
        F.datediff(F.col("insurance_valid_until"), F.current_date())
    ).withColumn(
        "insurance_status",
        F.when(F.col("insurance_days_to_expiry") < 0, "VENCIDO")
         .when(F.col("insurance_days_to_expiry") <= 90, "VENCENDO_EM_90_DIAS")
         .otherwise("ATIVO")
    ).withColumn(
        "insurance_days_expired",
        F.when(F.col("insurance_days_to_expiry") < 0, F.abs(F.col("insurance_days_to_expiry")))
         .otherwise(0)
    )
    
    print("      ‚úÖ Insurance KPIs calculados: insurance_status, insurance_days_expired")
    
    # Criar colunas de parti√ß√£o por data do evento
    df_silver_transformed = df_with_insurance_kpis.withColumn(
        "event_year",
        F.year(F.col("event_timestamp")).cast("string")
    ).withColumn(
        "event_month",
        F.lpad(F.month(F.col("event_timestamp")).cast("string"), 2, "0")
    ).withColumn(
        "event_day",
        F.lpad(F.dayofmonth(F.col("event_timestamp")).cast("string"), 2, "0")
    )
    
    print(f"   ‚úÖ Transforma√ß√£o completa! {df_silver_transformed.count()} registros transformados")
    
else:
    # Criar DataFrame vazio com schema esperado para casos sem dados novos
    print("   ‚ÑπÔ∏è  Criando DataFrame vazio com schema esperado...")
    df_silver_transformed = spark.createDataFrame([], schema=None)  # Schema ser√° inferido na pr√≥xima execu√ß√£o


# ============================================================================
# 4. CONSOLIDA√á√ÉO - CURRENT STATE (Estado Atual por Quilometragem)
# ============================================================================

print("\nÔøΩ ETAPA 3: Consolidando para estado atual por quilometragem...")

if new_records_count > 0:
    
    # Ler dados existentes na camada Silver
    print("   üìñ 1/3: Lendo dados existentes da camada Silver...")
    
    try:
        df_silver_existing = glueContext.create_dynamic_frame.from_catalog(
            database=args['silver_database'],
            table_name=args['silver_table']
        ).toDF()
        
        print(f"      ‚úÖ {df_silver_existing.count()} registros existentes encontrados")
    except Exception as e:
        print(f"      ‚ö†Ô∏è  Tabela n√£o existe ainda. Ser√° criada: {str(e)}")
        # Criar DataFrame vazio com schema igual aos novos dados
        df_silver_existing = spark.createDataFrame([], schema=None)  # Schema ser√° inferido
    
    # Unir dados novos + existentes
    print("   üîÑ 2/3: Combinando dados novos e existentes...")
    
    print(f"   Registros existentes: {df_silver_existing.count()}")
    print(f"   Registros novos: {df_silver_transformed.count()}")
    
    # Uni√£o dos dados (allowMissingColumns para compatibilidade de schema)
    if df_silver_existing.count() > 0:
        df_union = df_silver_transformed.unionByName(df_silver_existing, allowMissingColumns=True)
    else:
        df_union = df_silver_transformed
    
    print(f"   Total ap√≥s uni√£o: {df_union.count()}")
    
    # Aplicar l√≥gica de consolida√ß√£o: MAIOR QUILOMETRAGEM por chassis (estado mais atual)
    print("   üéØ 3/3: Aplicando consolida√ß√£o de estado atual por quilometragem...")
    
    # Determinar o registro com maior quilometragem para cada carChassis
    # current_mileage como crit√©rio principal + event_timestamp como desempate
    
    window_spec = Window.partitionBy("carChassis").orderBy(
        F.col("current_mileage").desc(),
        F.col("event_timestamp").desc()
    )
    
    df_current_state = df_union.withColumn(
        "row_number",
        F.row_number().over(window_spec)
    ).filter(
        F.col("row_number") == 1
    ).drop("row_number")
    
    print(f"   ‚úÖ Estado atual consolidado: {df_current_state.count()} ve√≠culos √∫nicos")
    
    # Estat√≠sticas de consolida√ß√£o
    total_records_before = df_union.count()
    unique_chassis_after = df_current_state.count()
    
    print(f"   üìä Consolida√ß√£o: {total_records_before} registros ‚Üí {unique_chassis_after} ve√≠culos √∫nicos")
    
    # Mostrar exemplo de consolida√ß√£o
    print("\n   üìã Exemplo de registros consolidados:")
    df_current_state.select(
        "carChassis",
        "current_mileage",
        "event_timestamp",
        "vehicle_manufacturer",
        "vehicle_model",
        "insurance_status",
        "insurance_days_expired"
    ).show(5, truncate=False)
    
else:
    print("   ‚ÑπÔ∏è  Nenhum registro novo para consolidar")
    df_current_state = spark.createDataFrame([], schema=None)

# ============================================================================
# 5. ESCRITA NO SILVER (DYNAMIC PARTITION OVERWRITE)
# ============================================================================

print("\nüíæ ETAPA 4: Escrevendo dados consolidados no Silver...")

if new_records_count > 0:
    
    print(f"   Bucket: {args['silver_bucket']}")
    print(f"   Path: {args['silver_path']}")
    
    # Escrever no S3 usando Spark DataFrame Writer (suporta Dynamic Partition Overwrite)
    # IMPORTANTE: Usar .write.mode("overwrite") com partitionOverwriteMode=dynamic
    # garante que apenas as parti√ß√µes afetadas sejam sobrescritas (n√£o todo o diret√≥rio)
    silver_output_path = f"s3://{args['silver_bucket']}/{args['silver_path']}"
    
    df_current_state.write \
        .mode("overwrite") \
        .partitionBy("event_year", "event_month", "event_day") \
        .format("parquet") \
        .option("compression", "snappy") \
        .save(silver_output_path)
    
    print(f"   ‚úÖ Dados escritos com sucesso!")
    print(f"   üì¶ Registros finais: {df_current_state.count()}")
    
    # Mostrar parti√ß√µes escritas
    partitions_written = df_current_state.select(
        "event_year", "event_month", "event_day"
    ).distinct().collect()
    
    print(f"\n   üìÇ Parti√ß√µes escritas ({len(partitions_written)}):")
    for partition in partitions_written:
        print(f"      - event_year={partition.event_year}/event_month={partition.event_month}/event_day={partition.event_day}")

else:
    print("   ‚ÑπÔ∏è  Nenhum registro novo para processar - Escrita pulada")

# ============================================================================
# 6. FINALIZA√á√ÉO DO JOB
# ============================================================================

print("\n" + "=" * 80)
print("‚úÖ JOB CONCLU√çDO COM SUCESSO!")
print("=" * 80)
print(f"üìä Resumo:")
print(f"   - Registros Bronze processados: {new_records_count}")
if new_records_count > 0:
    print(f"   - Registros Silver consolidados: {df_current_state.count()}")
    print(f"   - Ve√≠culos √∫nicos processados: {df_current_state.count()}")
    print(f"   - Taxa de consolida√ß√£o: {df_current_state.count()}/{new_records_count} = {round(df_current_state.count()/new_records_count*100, 1)}%")
print(f"   - Timestamp final: {datetime.now().isoformat()}")
print("=" * 80)

# Commit do Job (atualiza bookmarks)
job.commit()

print("\nüéØ Pr√≥ximos passos:")
print("   1. Executar Glue Crawler no Silver para atualizar cat√°logo")
print("   2. Consultar dados consolidados no Athena") 
print("   3. Verificar que Insurance KPIs est√£o funcionando")
print("   4. Validar consolida√ß√£o por current_mileage DESC")
