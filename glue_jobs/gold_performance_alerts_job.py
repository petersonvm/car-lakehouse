"""
AWS Glue Job: Gold Performance Alerts Log
=========================================

Objetivo:
    Criar tabela Gold com hist√≥rico de alertas de performance (KPIs cr√≠ticos).
    Filtra registros Silver que violam limites operacionais e classifica por tipo de alerta.

Camada: Gold (Agrega√ß√£o/Alertas)
Entrada: analytics_db.silver_car_telemetry
Sa√≠da: s3://[gold-bucket]/performance_alerts_log/ (append mode)

L√≥gica de Neg√≥cio (KPIs de Alerta):
    - KPI 2 (Temperatura Motor): engineTempCelsius > 100
    - KPI 2 (Temperatura √ìleo): oilTempCelsius > 110  
    - KPI 5 (Velocidade M√°xima): tripMaxSpeedKm > 110

Particionamento: alert_type, event_year, event_month, event_day

Job Bookmarks: Habilitado (processamento incremental)
Modo Escrita: Append (hist√≥rico cumulativo de infra√ß√µes)

Author: AWS Glue Agent
Date: 2025-10-31
"""

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql import functions as F
from pyspark.sql.types import StringType
from datetime import datetime
import logging

# ============================================================
# CONFIGURA√á√ÉO DE LOGGING
# ============================================================
logger = logging.getLogger()
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)

# ============================================================
# INICIALIZA√á√ÉO DO JOB GLUE
# ============================================================
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'gold_bucket',
    'glue_database'
])

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

logger.info("="*60)
logger.info(f"Job: {args['JOB_NAME']}")
logger.info(f"Gold Bucket: {args['gold_bucket']}")
logger.info(f"Database: {args['glue_database']}")
logger.info("="*60)

# ============================================================
# LEITURA INCREMENTAL - SILVER LAYER (COM JOB BOOKMARKS)
# ============================================================
logger.info("üìñ Lendo dados Silver com Job Bookmarks (incremental)...")

try:
    # Leitura incremental usando GlueContext e transformation_ctx
    dyf_silver = glueContext.create_dynamic_frame.from_catalog(
        database=args['glue_database'],
        table_name="silver_car_telemetry",
        transformation_ctx="read_silver_telemetry"  # Habilita Job Bookmarks
    )
    
    # Converter DynamicFrame para DataFrame Spark
    df_silver = dyf_silver.toDF()
    
    initial_count = df_silver.count()
    logger.info(f"‚úÖ Registros Silver lidos (novos desde √∫ltimo bookmark): {initial_count}")
    
    if initial_count == 0:
        logger.warning("‚ö†Ô∏è Nenhum dado novo para processar (Job Bookmark marca tudo como processado)")
        logger.info("Finalizando job com sucesso (sem novos alertas)")
        job.commit()
        sys.exit(0)
    
    logger.info(f"üìä Schema Silver: {df_silver.columns}")
    
except Exception as e:
    logger.error(f"‚ùå Erro ao ler dados Silver: {str(e)}")
    raise

# ============================================================
# L√ìGICA DE FILTRAGEM - KPIs DE ALERTA
# ============================================================
logger.info("üîç Aplicando filtros de KPIs de Performance...")

# Definir limites de alerta (KPIs)
KPI_ENGINE_TEMP_LIMIT = 100  # Celsius
KPI_OIL_TEMP_LIMIT = 110     # Celsius
KPI_MAX_SPEED_LIMIT = 120    # km/h

logger.info(f"   ‚Ä¢ KPI 2 - Temperatura Motor: > {KPI_ENGINE_TEMP_LIMIT}¬∞C")
logger.info(f"   ‚Ä¢ KPI 2 - Temperatura √ìleo: > {KPI_OIL_TEMP_LIMIT}¬∞C")
logger.info(f"   ‚Ä¢ KPI 5 - Velocidade M√°xima: > {KPI_MAX_SPEED_LIMIT} km/h")

# Filtro OR: qualquer viola√ß√£o gera um alerta
df_alerts = df_silver.filter(
    (F.col("metrics_engineTempCelsius") > KPI_ENGINE_TEMP_LIMIT) |
    (F.col("metrics_oilTempCelsius") > KPI_OIL_TEMP_LIMIT) |
    (F.col("metrics_trip_tripMaxSpeedKm") > KPI_MAX_SPEED_LIMIT)
)

alerts_count = df_alerts.count()
logger.info(f"‚ö†Ô∏è Total de ALERTAS detectados: {alerts_count}")

if alerts_count == 0:
    logger.info("‚úÖ Nenhum alerta detectado neste lote (todos os valores dentro dos limites)")
    logger.info("Finalizando job com sucesso (sem alertas)")
    job.commit()
    sys.exit(0)

# ============================================================
# ENRIQUECIMENTO - CLASSIFICA√á√ÉO DE ALERTAS
# ============================================================
logger.info("üè∑Ô∏è Classificando tipo de alerta...")

# UDF para determinar o tipo de alerta (pode ter m√∫ltiplos tipos por registro)
def classify_alert(engine_temp, oil_temp, max_speed):
    """
    Classifica o tipo de alerta baseado nas viola√ß√µes.
    Retorna o tipo mais cr√≠tico (prioridade: motor > √≥leo > velocidade).
    """
    if engine_temp is not None and engine_temp > KPI_ENGINE_TEMP_LIMIT:
        return "OVERHEAT_ENGINE"
    elif oil_temp is not None and oil_temp > KPI_OIL_TEMP_LIMIT:
        return "OVERHEAT_OIL"
    elif max_speed is not None and max_speed > KPI_MAX_SPEED_LIMIT:
        return "SPEEDING_ALERT"
    else:
        return "UNKNOWN"

classify_alert_udf = F.udf(classify_alert, StringType())

# Adicionar coluna alert_type
df_alerts_classified = df_alerts.withColumn(
    "alert_type",
    classify_alert_udf(
        F.col("metrics_engineTempCelsius"),
        F.col("metrics_oilTempCelsius"),
        F.col("metrics_trip_tripMaxSpeedKm")
    )
)

# Adicionar timestamp de processamento do alerta
df_alerts_classified = df_alerts_classified.withColumn(
    "alert_processed_timestamp",
    F.lit(datetime.now().isoformat())
)

# Estat√≠sticas por tipo de alerta
logger.info("üìä Distribui√ß√£o de alertas por tipo:")
alert_stats = df_alerts_classified.groupBy("alert_type").count().collect()
for row in alert_stats:
    logger.info(f"   ‚Ä¢ {row['alert_type']}: {row['count']} alertas")

# ============================================================
# ESCRITA - GOLD LAYER (APPEND MODE, PARTICIONADO)
# ============================================================
logger.info("üíæ Escrevendo alertas no Gold Layer...")

output_path = f"s3://{args['gold_bucket']}/performance_alerts_log/"
logger.info(f"Caminho de sa√≠da: {output_path}")
logger.info(f"Modo: APPEND (hist√≥rico cumulativo)")
logger.info(f"Particionamento: alert_type, event_year, event_month, event_day")

try:
    # Escrever no formato Parquet com particionamento
    df_alerts_classified.write \
        .mode("append") \
        .partitionBy("alert_type", "event_year", "event_month", "event_day") \
        .parquet(output_path)
    
    logger.info("‚úÖ Alertas escritos com sucesso no Gold!")
    logger.info(f"üìä Total de alertas persistidos: {alerts_count}")
    
except Exception as e:
    logger.error(f"‚ùå Erro ao escrever no Gold: {str(e)}")
    raise

# ============================================================
# ESTAT√çSTICAS FINAIS
# ============================================================
logger.info("="*60)
logger.info("üìà ESTAT√çSTICAS FINAIS DO JOB")
logger.info("="*60)
logger.info(f"Registros Silver processados: {initial_count}")
logger.info(f"Alertas gerados: {alerts_count}")
logger.info(f"Taxa de viola√ß√£o: {(alerts_count/initial_count*100):.2f}%")
logger.info(f"Destino: {output_path}")
logger.info("="*60)

# Commit do Job (atualiza Job Bookmark)
job.commit()
logger.info("‚úÖ Job finalizado com sucesso!")
