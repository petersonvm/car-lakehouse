"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
AWS GLUE ETL JOB: Gold Performance Alerts - SLIM (Optimized)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ DESCRIÃ‡ÃƒO:
    Pipeline ETL OTIMIZADO para detectar alertas de performance de veÃ­culos.
    
    **DIFERENÃ‡A CRÃTICA vs. Pipeline Antigo:**
    - âŒ Antigo: Copiava ~36 colunas (Model, Manufacturer, market_price, etc.)
    - âœ… Novo: Seleciona APENAS 7 colunas essenciais (Quem, Quando, O QuÃª, Valor)
    - ğŸ“Š ReduÃ§Ã£o: ~80% menos armazenamento e custo

ğŸ“¥ INPUT:
    - Source: silver_car_telemetry (Glue Catalog)
    - Mode: Incremental (Job Bookmarks)
    - Database: datalake-pipeline-catalog-dev

ğŸ“¤ OUTPUT:
    - Target: s3://[gold-bucket]/performance_alerts_log_slim/
    - Format: Parquet
    - Mode: Append
    - Partitions: alert_type, event_year, event_month, event_day

ğŸ” KPIs MONITORADOS:
    1. OVERHEAT_ENGINE: engineTempCelsius > 100Â°C
    2. OVERHEAT_OIL: oilTempCelsius > 110Â°C
    3. SPEEDING_ALERT: tripMaxSpeedKm > 110 km/h

ğŸ“Š SCHEMA (SLIM):
    â”œâ”€â”€ carChassis (string) - Identificador do veÃ­culo
    â”œâ”€â”€ metrics_metricTimestamp (string) - Quando ocorreu
    â”œâ”€â”€ alert_type (string) - Tipo do alerta
    â”œâ”€â”€ alert_value (double) - Valor que disparou o alerta
    â”œâ”€â”€ event_year (string) - PartiÃ§Ã£o
    â”œâ”€â”€ event_month (string) - PartiÃ§Ã£o
    â””â”€â”€ event_day (string) - PartiÃ§Ã£o

ğŸ¯ OTIMIZAÃ‡Ã•ES APLICADAS:
    - âœ… .select() para apenas colunas necessÃ¡rias
    - âœ… Particionamento multi-nÃ­vel (alert_type + data)
    - âœ… Job Bookmarks para processamento incremental
    - âœ… Sem duplicaÃ§Ã£o de dados do Silver Layer

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.sql.functions import col, when, lit, current_timestamp
from datetime import datetime

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. INICIALIZAÃ‡ÃƒO DO JOB
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Capturar argumentos passados pelo Job
args = getResolvedOptions(sys.argv, [
    'JOB_NAME',
    'glue_database',
    'gold_bucket'
])

# Inicializar contextos
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# VariÃ¡veis de ambiente
GLUE_DATABASE = args['glue_database']
GOLD_BUCKET = args['gold_bucket']
OUTPUT_PATH = f"s3://{GOLD_BUCKET}/performance_alerts_log_slim/"

print("â•" * 80)
print("ğŸš€ GOLD PERFORMANCE ALERTS - SLIM PIPELINE")
print("â•" * 80)
print(f"ğŸ“Š Database: {GLUE_DATABASE}")
print(f"ğŸ“ Output Path: {OUTPUT_PATH}")
print(f"â° Execution Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("â•" * 80)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. LEITURA INCREMENTAL (Job Bookmarks)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ“¥ STEP 1: Reading INCREMENTAL data from Silver Layer...")

try:
    # Leitura incremental com transformation_ctx (habilita Job Bookmarks)
    dyf_silver_telemetry = glueContext.create_dynamic_frame.from_catalog(
        database=GLUE_DATABASE,
        table_name="silver_car_telemetry",
        transformation_ctx="read_silver_telemetry_incremental_slim"
    )
    
    # Converter para Spark DataFrame para operaÃ§Ãµes avanÃ§adas
    df_silver = dyf_silver_telemetry.toDF()
    
    print(f"âœ… Incremental read completed")
    print(f"   Records to process: {df_silver.count()}")
    
except Exception as e:
    print(f"âŒ ERROR reading Silver data: {str(e)}")
    raise

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. FILTRAGEM (KPIs de Performance)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ” STEP 2: Filtering performance violations...")

try:
    # Filtrar apenas registros que violaram pelo menos 1 KPI
    df_alerts = df_silver.filter(
        (col("metrics_engineTempCelsius") > 100) |
        (col("metrics_oilTempCelsius") > 110) |
        (col("metrics_trip_tripMaxSpeedKm") > 110)
    )
    
    alerts_count = df_alerts.count()
    print(f"âœ… Filtering completed")
    print(f"   ğŸš¨ Alerts detected: {alerts_count}")
    
    if alerts_count == 0:
        print("   â„¹ï¸  No performance violations found in this batch")
        print("   âœ… Job completed successfully (no data to write)")
        job.commit()
        sys.exit(0)
    
except Exception as e:
    print(f"âŒ ERROR filtering alerts: {str(e)}")
    raise

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. ENRIQUECIMENTO (ClassificaÃ§Ã£o + Valor do Alerta)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nğŸ·ï¸  STEP 3: Enriching with alert_type and alert_value...")

try:
    # Criar coluna alert_type (classificaÃ§Ã£o do tipo de alerta)
    df_enriched = df_alerts.withColumn(
        "alert_type",
        when(col("metrics_engineTempCelsius") > 100, lit("OVERHEAT_ENGINE"))
        .when(col("metrics_oilTempCelsius") > 110, lit("OVERHEAT_OIL"))
        .when(col("metrics_trip_tripMaxSpeedKm") > 110, lit("SPEEDING_ALERT"))
        .otherwise(lit("UNKNOWN"))
    )
    
    # Criar coluna alert_value (captura o valor que disparou o alerta)
    df_enriched = df_enriched.withColumn(
        "alert_value",
        when(col("metrics_engineTempCelsius") > 100, col("metrics_engineTempCelsius"))
        .when(col("metrics_oilTempCelsius") > 110, col("metrics_oilTempCelsius"))
        .when(col("metrics_trip_tripMaxSpeedKm") > 110, col("metrics_trip_tripMaxSpeedKm"))
        .otherwise(lit(None))
    )
    
    print("âœ… Enrichment completed")
    print(f"   Columns added: alert_type, alert_value")
    
    # Log da distribuiÃ§Ã£o de tipos de alertas
    print("\n   ğŸ“Š Alert Distribution:")
    alert_distribution = df_enriched.groupBy("alert_type").count().collect()
    for row in alert_distribution:
        print(f"      â€¢ {row['alert_type']}: {row['count']} alerts")
    
except Exception as e:
    print(f"âŒ ERROR enriching data: {str(e)}")
    raise

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. SELEÃ‡ÃƒO (SLIM - Apenas Colunas Essenciais)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\nâœ‚ï¸  STEP 4: Selecting SLIM columns (optimization)...")

try:
    # âš ï¸ CRÃTICO: Selecionar APENAS as 7 colunas essenciais
    # Esta Ã© a principal diferenÃ§a vs. pipeline antigo (que tinha ~36 colunas)
    df_slim = df_enriched.select(
        col("carChassis"),                      # Quem (identificador do veÃ­culo)
        col("metrics_metricTimestamp"),         # Quando (timestamp do evento)
        col("alert_type"),                      # O QuÃª (tipo do alerta)
        col("alert_value"),                     # Valor (valor que disparou)
        col("event_year"),                      # PartiÃ§Ã£o 1
        col("event_month"),                     # PartiÃ§Ã£o 2
        col("event_day")                        # PartiÃ§Ã£o 3
    )
    
    print("âœ… Slim selection completed")
    print(f"   Total columns: {len(df_slim.columns)} (vs 36 in old pipeline)")
    print(f"   Columns: {', '.join(df_slim.columns)}")
    print(f"   ğŸ’° Storage reduction: ~80%")
    
except Exception as e:
    print(f"âŒ ERROR selecting columns: {str(e)}")
    raise

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. ESCRITA (Append com Particionamento)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(f"\nğŸ’¾ STEP 5: Writing SLIM alerts to Gold Layer...")
print(f"   ğŸ“ Target: {OUTPUT_PATH}")
print(f"   ğŸ“¦ Mode: append")
print(f"   ğŸ—‚ï¸  Partitions: alert_type, event_year, event_month, event_day")

try:
    # Escrever no S3 com particionamento multi-nÃ­vel
    df_slim.write \
        .mode("append") \
        .partitionBy("alert_type", "event_year", "event_month", "event_day") \
        .parquet(OUTPUT_PATH)
    
    print("âœ… Write completed successfully")
    print(f"   Records written: {df_slim.count()}")
    
except Exception as e:
    print(f"âŒ ERROR writing to S3: {str(e)}")
    raise

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 7. FINALIZAÃ‡ÃƒO
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "â•" * 80)
print("âœ… JOB COMPLETED SUCCESSFULLY")
print("â•" * 80)
print(f"ğŸ“Š Summary:")
print(f"   â€¢ Total alerts processed: {df_slim.count()}")
print(f"   â€¢ Alert types: {df_enriched.select('alert_type').distinct().count()}")
print(f"   â€¢ Output location: {OUTPUT_PATH}")
print(f"   â€¢ Schema: SLIM (7 columns only)")
print(f"   â€¢ Mode: Incremental (Job Bookmarks enabled)")
print("â•" * 80)

# Commit do Job (salvar estado do Bookmark)
job.commit()
