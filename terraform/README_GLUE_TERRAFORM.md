# Terraform - Infraestrutura AWS Glue Job

## üìÅ Estrutura de Arquivos

```
terraform/
‚îú‚îÄ‚îÄ glue_jobs.tf                    # ‚úÖ NOVO - Infraestrutura Glue Job
‚îú‚îÄ‚îÄ lambda.tf                       # üîÑ MODIFICADO - Removido S3 trigger cleansing
‚îú‚îÄ‚îÄ variables.tf                    # üîÑ MODIFICADO - Adicionadas vari√°veis Glue
‚îú‚îÄ‚îÄ s3.tf                          # ‚ö™ SEM ALTERA√á√ïES
‚îú‚îÄ‚îÄ glue.tf                        # ‚ö™ SEM ALTERA√á√ïES (crawlers)
‚îú‚îÄ‚îÄ iam.tf                         # ‚ö™ SEM ALTERA√á√ïES
‚îú‚îÄ‚îÄ athena.tf                      # ‚ö™ SEM ALTERA√á√ïES
‚îú‚îÄ‚îÄ outputs.tf                     # ‚ö™ SEM ALTERA√á√ïES
‚îú‚îÄ‚îÄ provider.tf                    # ‚ö™ SEM ALTERA√á√ïES
‚îú‚îÄ‚îÄ deploy_glue_migration.ps1      # ‚úÖ NOVO - Script de deploy automatizado
‚îî‚îÄ‚îÄ terraform-backups/             # ‚úÖ NOVO - Diret√≥rio para backups
```

## üéØ O que foi criado: `glue_jobs.tf`

### Recursos Provisionados

#### 1Ô∏è‚É£ **S3 Buckets**
```hcl
aws_s3_bucket.glue_scripts       # Scripts PySpark
aws_s3_bucket.glue_temp          # Arquivos tempor√°rios do Glue
```

**Prop√≥sito:**
- `glue_scripts`: Armazena o script `silver_consolidation_job.py`
- `glue_temp`: Usado pelo Glue para arquivos tempor√°rios durante execu√ß√£o

**Configura√ß√µes:**
- ‚úÖ Versionamento habilitado (scripts)
- ‚úÖ Block Public Access
- ‚úÖ Lifecycle policy (temp bucket: delete ap√≥s 7 dias)

#### 2Ô∏è‚É£ **Upload do Script**
```hcl
aws_s3_object.silver_consolidation_script
```

**Prop√≥sito:**
- Faz upload do script PySpark local para o S3
- Detecta mudan√ßas via `filemd5()` (re-upload autom√°tico)

**Path local:** `../glue_jobs/silver_consolidation_job.py`  
**Path S3:** `s3://datalake-pipeline-glue-scripts-dev/glue_jobs/silver_consolidation_job.py`

#### 3Ô∏è‚É£ **IAM Role para Glue**
```hcl
aws_iam_role.glue_job                        # Role principal
aws_iam_role_policy.glue_s3_access          # Acesso S3
aws_iam_role_policy.glue_catalog_access     # Acesso Glue Catalog
aws_iam_role_policy.glue_cloudwatch_logs    # Acesso CloudWatch
```

**Pol√≠ticas Implementadas:**

| Pol√≠tica | Recursos | Permiss√µes |
|----------|----------|------------|
| **glue_s3_access** | Bronze bucket | `s3:GetObject`, `s3:ListBucket` |
|  | Silver bucket | `s3:GetObject`, `s3:PutObject`, `s3:DeleteObject`, `s3:ListBucket` |
|  | Scripts bucket | `s3:GetObject`, `s3:ListBucket` |
|  | Temp bucket | `s3:*` |
| **glue_catalog_access** | Glue Database | `glue:Get*`, `glue:Create*`, `glue:Update*`, `glue:Delete*` |
|  | Glue Tables | `glue:*Partition*`, `glue:*Table*` |
| **glue_cloudwatch_logs** | CloudWatch Logs | `logs:CreateLogGroup`, `logs:CreateLogStream`, `logs:PutLogEvents` |

**Princ√≠pio de Menor Privil√©gio:**
- ‚úÖ Bronze: **Somente leitura** (input imut√°vel)
- ‚úÖ Silver: **Leitura/Escrita/Delete** (necess√°rio para Dynamic Partition Overwrite)
- ‚úÖ Gold: **Sem acesso** (n√£o usado neste job)

#### 4Ô∏è‚É£ **AWS Glue Job**
```hcl
aws_glue_job.silver_consolidation
```

**Configura√ß√£o:**

| Par√¢metro | Valor | Descri√ß√£o |
|-----------|-------|-----------|
| `name` | `datalake-pipeline-silver-consolidation-dev` | Nome do Job |
| `role_arn` | IAM Role criada | Permiss√µes de execu√ß√£o |
| `glue_version` | `4.0` | Spark 3.3, Python 3 |
| `command.name` | `glueetl` | Tipo de Job (ETL) |
| `command.script_location` | S3 path do script | Script PySpark |
| `worker_type` | `G.1X` | 4 vCPU, 16 GB RAM |
| `number_of_workers` | `2` | 2 workers (escal√°vel) |
| `timeout` | `60` min | Timeout m√°ximo |
| `max_retries` | `1` | Tentativas em caso de falha |

**Argumentos do Job (`default_arguments`):**

```hcl
--job-bookmark-option              = "job-bookmark-enable"
--enable-glue-datacatalog          = "true"
--enable-metrics                   = "true"
--enable-continuous-cloudwatch-log = "true"
--enable-spark-ui                  = "true"
--conf                             = "spark.sql.sources.partitionOverwriteMode=dynamic"
--TempDir                          = "s3://glue-temp-bucket/temp/"
--enable-auto-scaling              = "true"

# Par√¢metros customizados do script
--bronze_database  = "datalake-pipeline-catalog-dev"
--bronze_table     = "bronze_ingest_year_2025"
--silver_database  = "datalake-pipeline-catalog-dev"
--silver_table     = "silver_car_telemetry"
--silver_bucket    = "datalake-pipeline-silver-dev"
--silver_path      = "car_telemetry/"
```

**Argumentos Cr√≠ticos:**

- `--job-bookmark-enable`: Habilita processamento incremental (apenas arquivos novos)
- `--conf spark.sql.sources.partitionOverwriteMode=dynamic`: Essencial para Upsert (sobrescreve apenas parti√ß√µes afetadas)
- `--enable-auto-scaling`: Workers escalados automaticamente conforme carga

#### 5Ô∏è‚É£ **Glue Trigger (Agendamento)**
```hcl
aws_glue_trigger.silver_consolidation_schedule
```

**Configura√ß√£o:**

| Par√¢metro | Valor | Descri√ß√£o |
|-----------|-------|-----------|
| `type` | `SCHEDULED` | Trigger baseado em schedule |
| `schedule` | `cron(0 */1 * * ? *)` | A cada hora |
| `enabled` | `true` | Habilitado por padr√£o |

**Formato Cron AWS:**
```
cron(Minutes Hours Day-of-month Month Day-of-week Year)
      ‚Üì       ‚Üì         ‚Üì         ‚Üì        ‚Üì        ‚Üì
     0-59   0-23      1-31      1-12    SUN-SAT    *
```

**Exemplos:**
- `cron(0 */1 * * ? *)` ‚Üí A cada hora
- `cron(0 */2 * * ? *)` ‚Üí A cada 2 horas
- `cron(0 0 * * ? *)` ‚Üí Diariamente √† meia-noite UTC
- `cron(0 2 * * ? *)` ‚Üí Diariamente √†s 2 AM UTC
- `cron(0 0 ? * MON *)` ‚Üí Semanalmente √†s segundas-feiras

#### 6Ô∏è‚É£ **CloudWatch Log Group**
```hcl
aws_cloudwatch_log_group.glue_job_logs
```

**Configura√ß√£o:**
- Nome: `/aws-glue/jobs/datalake-pipeline-silver-consolidation-dev`
- Reten√ß√£o: 14 dias
- Criado antecipadamente (evita cria√ß√£o autom√°tica sem reten√ß√£o)

## üîß Vari√°veis Configur√°veis

### Arquivo: `variables.tf`

```hcl
# Script PySpark
variable "glue_silver_script_path" {
  default = "../glue_jobs/silver_consolidation_job.py"
}

# Vers√£o do Glue (Spark + Python)
variable "glue_version" {
  default = "4.0"  # Spark 3.3, Python 3
}

# Capacidade de processamento
variable "glue_worker_type" {
  default = "G.1X"  # 4 vCPU, 16 GB RAM
  # Op√ß√µes: "G.025X", "G.1X", "G.2X", "G.4X", "G.8X"
}

variable "glue_number_of_workers" {
  default = 2
  # Ajustar conforme volume de dados
}

# Timeout e retries
variable "glue_job_timeout_minutes" {
  default = 60  # 1 hora
}

variable "glue_job_max_retries" {
  default = 1
}

variable "glue_max_concurrent_runs" {
  default = 1  # Evita execu√ß√µes sobrepostas
}

# Agendamento
variable "glue_trigger_schedule" {
  default = "cron(0 */1 * * ? *)"  # Hor√°rio
}

variable "glue_trigger_enabled" {
  default = true
}

# Nomes de tabelas
variable "bronze_table_name" {
  default = "bronze_ingest_year_2025"
}

variable "silver_table_name" {
  default = "silver_car_telemetry"
}

variable "silver_path" {
  default = "car_telemetry/"
}

# Logs
variable "cloudwatch_log_retention_days" {
  default = 14
}
```

## üóëÔ∏è O que foi removido: `lambda.tf`

### Recursos Deletados

```hcl
# ‚ùå REMOVIDO
resource "aws_lambda_permission" "allow_s3_invoke_cleansing" { ... }

# ‚ùå REMOVIDO
resource "aws_s3_bucket_notification" "bronze_bucket_notification" { ... }
```

**Por que removidos?**
- Lambda Cleansing n√£o √© mais acionada por eventos S3
- Glue Job roda em schedule (n√£o precisa de S3 trigger)
- Lambda Cleansing function **mantida** (pode ser √∫til para debug/fallback)

## üöÄ Como Usar

### Op√ß√£o 1: Script Automatizado (Recomendado)

```powershell
cd terraform
.\deploy_glue_migration.ps1
```

**O script faz:**
1. ‚úÖ Valida pr√©-requisitos (Terraform, AWS CLI, credenciais)
2. ‚úÖ Verifica sintaxe do script PySpark
3. ‚úÖ Cria backup do estado Terraform
4. ‚úÖ Executa `terraform plan` e mostra mudan√ßas
5. ‚úÖ Pede confirma√ß√£o manual
6. ‚úÖ Executa `terraform apply`
7. ‚úÖ Valida recursos criados (Glue Job, Trigger, IAM Role, etc.)
8. ‚úÖ Exibe resumo e pr√≥ximos passos

### Op√ß√£o 2: Manual

```bash
# 1. Inicializar (se necess√°rio)
terraform init

# 2. Validar configura√ß√£o
terraform validate

# 3. Ver plano de execu√ß√£o
terraform plan

# 4. Aplicar mudan√ßas
terraform apply

# 5. Verificar outputs
terraform output
```

## üß™ Valida√ß√£o P√≥s-Deploy

### 1. Verificar Glue Job

```bash
aws glue get-job --job-name datalake-pipeline-silver-consolidation-dev
```

**Output esperado:**
```json
{
  "Job": {
    "Name": "datalake-pipeline-silver-consolidation-dev",
    "Role": "arn:aws:iam::...:role/datalake-pipeline-glue-job-role-dev",
    "Command": {
      "Name": "glueetl",
      "ScriptLocation": "s3://datalake-pipeline-glue-scripts-dev/glue_jobs/silver_consolidation_job.py"
    },
    "GlueVersion": "4.0"
  }
}
```

### 2. Verificar Trigger

```bash
aws glue get-trigger --name datalake-pipeline-silver-consolidation-trigger-dev
```

**Output esperado:**
```json
{
  "Trigger": {
    "Name": "datalake-pipeline-silver-consolidation-trigger-dev",
    "Type": "SCHEDULED",
    "State": "ACTIVATED",
    "Schedule": "cron(0 */1 * * ? *)"
  }
}
```

### 3. Verificar Script no S3

```bash
aws s3 ls s3://datalake-pipeline-glue-scripts-dev/glue_jobs/
```

**Output esperado:**
```
2025-10-30 15:00:00      12345 silver_consolidation_job.py
```

### 4. Teste Manual do Job

```bash
# Iniciar job
aws glue start-job-run --job-name datalake-pipeline-silver-consolidation-dev

# Verificar status
aws glue get-job-runs --job-name datalake-pipeline-silver-consolidation-dev --max-results 1
```

### 5. Monitorar Logs

```bash
# Tail dos logs (real-time)
aws logs tail /aws-glue/jobs/datalake-pipeline-silver-consolidation-dev --follow

# √öltimos 100 logs
aws logs tail /aws-glue/jobs/datalake-pipeline-silver-consolidation-dev --since 1h
```

## üîÑ Atualiza√ß√µes Futuras

### Atualizar Script PySpark

```bash
# 1. Modificar script local
vim ../glue_jobs/silver_consolidation_job.py

# 2. Terraform detecta mudan√ßa via filemd5()
terraform plan
# Output: aws_s3_object.silver_consolidation_script will be updated in-place

# 3. Aplicar
terraform apply
# O Glue Job usar√° o novo script na pr√≥xima execu√ß√£o
```

### Ajustar Frequ√™ncia do Job

```hcl
# terraform.tfvars
glue_trigger_schedule = "cron(0 */2 * * ? *)"  # A cada 2 horas
```

```bash
terraform apply
```

### Desabilitar Trigger (sem deletar Job)

```bash
terraform apply -var="glue_trigger_enabled=false"
```

### Aumentar Capacidade de Processamento

```hcl
# terraform.tfvars
glue_worker_type      = "G.2X"  # 8 vCPU, 32 GB RAM
glue_number_of_workers = 5
```

```bash
terraform apply
```

## üí∞ Estimativa de Custos

### Configura√ß√£o Padr√£o
- Worker Type: G.1X (4 vCPU, 16 GB RAM)
- Workers: 2
- Execu√ß√µes: 24/dia (hor√°rio)
- Dura√ß√£o m√©dia: 5 minutos/execu√ß√£o

### C√°lculo Mensal (us-east-1)

```
DPU-hours = 24 execu√ß√µes/dia √ó 30 dias √ó 5 min √ó 2 DPUs = 120 DPU-hours
Custo = 120 DPU-hours √ó $0,44/DPU-hour = $52,80/m√™s
```

### Otimiza√ß√µes de Custo

| Mudan√ßa | Custo Mensal | Economia |
|---------|--------------|----------|
| **Configura√ß√£o Atual** (hor√°rio, G.1X, 2 workers) | **$52,80** | - |
| Reduzir para 2√ó/dia | $4,40 | 92% |
| Usar G.025X (se volume baixo) | $26,40 | 50% |
| Reduzir para 1 worker | $26,40 | 50% |
| Combo: 1√ó/dia + G.025X + 1 worker | $0,66 | 99% |

### S3 e Outros Custos

- **Glue Scripts Bucket**: ~$0,01/m√™s (< 1 MB)
- **Glue Temp Bucket**: ~$0,10/m√™s (lifecycle 7 dias)
- **CloudWatch Logs**: ~$0,50/m√™s (14 dias reten√ß√£o)
- **Glue Data Catalog**: Gratuito (< 1M objetos)

**Total estimado:** ~$53,50/m√™s

## üìä Outputs Terraform

Ap√≥s `terraform apply`, os seguintes outputs est√£o dispon√≠veis:

```bash
terraform output
```

```hcl
glue_job_name = "datalake-pipeline-silver-consolidation-dev"
glue_job_arn = "arn:aws:glue:us-east-1:123456789012:job/datalake-pipeline-silver-consolidation-dev"
glue_trigger_name = "datalake-pipeline-silver-consolidation-trigger-dev"
glue_scripts_bucket = "datalake-pipeline-glue-scripts-dev"
glue_job_role_arn = "arn:aws:iam::123456789012:role/datalake-pipeline-glue-job-role-dev"
```

## üÜò Troubleshooting

### Erro: "Script not found in S3"

**Causa:** Script local n√£o existe ou path incorreto

**Solu√ß√£o:**
```bash
# Verificar path
ls ../glue_jobs/silver_consolidation_job.py

# Ajustar vari√°vel se necess√°rio
terraform apply -var="glue_silver_script_path=/caminho/correto/silver_consolidation_job.py"
```

### Erro: "AccessDeniedException" ao executar Job

**Causa:** IAM Role sem permiss√µes suficientes

**Solu√ß√£o:**
```bash
# Verificar pol√≠ticas
terraform state show aws_iam_role_policy.glue_s3_access

# Recriar pol√≠ticas
terraform taint aws_iam_role_policy.glue_s3_access
terraform apply
```

### Erro: Job Bookmarks n√£o funcionam

**Causa:** Par√¢metro `--job-bookmark-option` incorreto

**Solu√ß√£o:**
```bash
# Verificar argumentos do Job
aws glue get-job --job-name datalake-pipeline-silver-consolidation-dev \
  --query 'Job.DefaultArguments."--job-bookmark-option"'

# Deve retornar: "job-bookmark-enable"
```

### Trigger n√£o executa Job

**Causa:** Trigger desabilitado ou schedule incorreto

**Solu√ß√£o:**
```bash
# Verificar estado
aws glue get-trigger --name datalake-pipeline-silver-consolidation-trigger-dev

# Habilitar manualmente
aws glue start-trigger --name datalake-pipeline-silver-consolidation-trigger-dev
```

## üìö Refer√™ncias

- [Terraform AWS Glue Job](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/glue_job)
- [Terraform AWS Glue Trigger](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/glue_trigger)
- [AWS Glue Job Parameters](https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-glue-arguments.html)
- [AWS Glue Job Bookmarks](https://docs.aws.amazon.com/glue/latest/dg/monitor-continuations.html)

---

**Autor**: Sistema de Data Lakehouse  
**Data**: 2025-10-30  
**Vers√£o**: 1.0
