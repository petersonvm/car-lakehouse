# Implementa√ß√£o de Corre√ß√µes - Gold Layer Iceberg

**Data:** 13 de Novembro de 2025  
**Branch:** ice  
**Status:** PRONTO PARA TESTE

---

## üéØ Objetivo

Implementar corre√ß√µes avan√ßadas nos Jobs Gold para for√ßar materializa√ß√£o do schema correto e eliminar o problema de colunas da Silver sendo inclu√≠das na Gold.

---

## ‚úÖ Altera√ß√µes Implementadas

### 1. **Scripts PySpark - Todos os 3 Jobs Gold**

Arquivos modificados:
- `glue_jobs/gold_car_current_state_job_iceberg.py`
- `glue_jobs/gold_fuel_efficiency_job_iceberg.py`
- `glue_jobs/gold_performance_alerts_job_iceberg.py`

#### A. Habilita√ß√£o de Checkpointing (Linha ~73)
```python
# Configure Spark checkpoint directory (for forced materialization)
print("\n  Configuring Spark checkpoint directory...")
spark.sparkContext.setCheckpointDir("s3://datalake-pipeline-glue-temp-dev/spark-checkpoints/gold/")
print("  ‚úÖ Checkpoint directory set")
```

**Objetivo:** Configurar diret√≥rio S3 para checkpoint do Spark, permitindo quebra de linhagem.

#### B. Drop Expl√≠cito de Colunas Silver (Novo Step 2.5)
```python
# ============================================================================
# 3.5. DROP UNWANTED COLUMNS (EXPLICIT REMOVAL)
# ============================================================================

print("\n" + "=" * 80)
print(" STEP 2.5: Dropping Silver-inherited columns")
print("=" * 80)

# Explicitly drop Silver partition columns and Silver-only metadata
df_cleaned = df_with_kpis.drop(
    "event_id",               # Silver primary key (not needed in Gold)
    "event_primary_timestamp", # Silver metadata (not needed in Gold)
    "event_year",              # Silver partition column
    "event_month",             # Silver partition column
    "event_day"                # Silver partition column
)

print("\n  ‚úÖ Dropped 5 Silver-inherited columns: event_id, event_primary_timestamp, event_year, event_month, event_day")
```

**Objetivo:** Remover explicitamente **ANTES** do `.select()` todas as colunas herdadas da Silver que n√£o devem existir na Gold.

#### C. Sele√ß√£o de Colunas Gold (Step 2.5 continua√ß√£o)
```python
# Select only Gold columns from cleaned DataFrame
print("\n  Selecting final Gold layer columns from cleaned DataFrame...")
df_gold = df_cleaned.select(
    "car_chassis",
    "manufacturer",
    "model",
    "year",
    "gas_type",
    "insurance_provider",
    "insurance_valid_until",
    "current_mileage_km",
    "fuel_available_liters",
    "telemetry_timestamp",
    "insurance_status",
    "insurance_days_expired",
    "event_timestamp",
    "gold_processing_timestamp"
)
```

**Objetivo:** Selecionar apenas as 14 colunas corretas do DataFrame **J√Å LIMPO**.

#### D. For√ßar Materializa√ß√£o via Checkpoint (Novo Step 2.6)
```python
# ============================================================================
# 3.6. FORCE MATERIALIZATION (CHECKPOINT - BREAKS LINEAGE)
# ============================================================================

print("\n  Checkpointing df_gold to break Spark lineage and force schema materialization...")
print("  This ensures Iceberg receives the EXACT schema defined above (14 columns)")

# CRITICAL: checkpoint() forces Spark to materialize the DataFrame and breaks lineage
# This prevents Iceberg from "looking back" at parent DataFrames (df_with_kpis)
df_gold = df_gold.checkpoint()

gold_count = df_gold.count()
print(f"\n  ‚úÖ Checkpoint completed: {gold_count} rows materialized")
```

**Objetivo:** **QUEBRAR A LINHAGEM DO SPARK**. O `checkpoint()` for√ßa materializa√ß√£o f√≠sica do DataFrame em S3 e elimina refer√™ncias aos DataFrames pais (df_with_kpis, df_silver). O Iceberg n√£o pode mais "olhar para tr√°s" e usar schema anterior.

#### E. Valida√ß√£o de Schema (Novo Step 2.7)
```python
# ============================================================================
# 3.7. SCHEMA VALIDATION (DEBUGGING)
# ============================================================================

print("\n  Final schema being sent to Iceberg:")
df_gold.printSchema()

print(f"\n  Final column list ({len(df_gold.columns)} columns):")
for idx, col_name in enumerate(df_gold.columns, 1):
    print(f"    {idx}. {col_name}")

print("\n  ‚ö†Ô∏è  CRITICAL CHECK: Verify NO Silver columns (event_id, event_year, etc.)")
silver_only_cols = ["event_id", "event_primary_timestamp", "event_year", "event_month", "event_day"]
found_silver_cols = [col for col in silver_only_cols if col in df_gold.columns]

if found_silver_cols:
    error_msg = f"‚ùå ERROR: Silver columns still present in df_gold: {found_silver_cols}"
    print(f"\n  {error_msg}")
    raise ValueError(error_msg)
else:
    print("\n  ‚úÖ VALIDATION PASSED: No Silver-only columns in df_gold")
```

**Objetivo:** Validar com **erro fatal** se alguma coluna Silver ainda est√° presente. Logging detalhado para diagn√≥stico.

#### F. Cria√ß√£o da Tabela Iceberg
```python
# Create/Replace Iceberg table using writeTo() API
print("\n  Calling writeTo().createOrReplace()...")
print("  (Using checkpointed DataFrame with materialized schema)")

df_gold.writeTo(gold_table) \
    .using("iceberg") \
    .tableProperty("format-version", "2") \
    .createOrReplace()

print("\n  ‚úÖ Table created/replaced successfully")
```

**Objetivo:** Usar API `writeTo()` com DataFrame **checkpointed** (sem linhagem).

---

### 2. **Infraestrutura Terraform**

Arquivo modificado:
- `terraform/iceberg_migration.tf`

#### A. Spark UI Path Atualizado (3 jobs)
```terraform
"--spark-event-logs-path" = "s3://${aws_s3_bucket.glue_temp.bucket}/spark-ui-logs/"
```

**Mudan√ßa:** `spark-logs/` ‚Üí `spark-ui-logs/` (mais espec√≠fico)

**Objetivo:** Logs organizados para an√°lise do plano de execu√ß√£o Spark.

#### B. Workflow Sequential (J√Å IMPLEMENTADO)

Arquivo: `terraform/iceberg_event_driven.tf`

**Triggers configurados:**
1. **Trigger 1:** Silver SUCCESS ‚Üí Gold Job 1 (Car Current State)
2. **Trigger 2:** Gold Job 1 SUCCESS ‚Üí Gold Job 2 (Fuel Efficiency)
3. **Trigger 3:** Gold Job 2 SUCCESS ‚Üí Gold Job 3 (Performance Alerts)

**Status:** ‚úÖ J√° estava implementado (Fase 18)

---

## üî¨ Teoria da Corre√ß√£o

### Problema Identificado
O Iceberg `writeTo().createOrReplace()` estava usando o schema do DataFrame **pai** (`df_with_kpis` com 19 colunas) ao inv√©s do DataFrame **derivado** (`df_gold` com 14 colunas).

### Causa Raiz
**Lazy Evaluation do Spark + Lineage Tracking:**
- Spark mant√©m linhagem de transforma√ß√µes sem execut√°-las
- Iceberg pode estar inspecionando a linhagem e inferindo schema do DataFrame fonte (df_with_kpis)
- `.cache()` n√£o quebra linhagem (apenas marca para materializa√ß√£o em mem√≥ria)
- `.select()` √© uma transforma√ß√£o l√≥gica, n√£o quebra linhagem

### Solu√ß√£o Implementada
**For√ßar Materializa√ß√£o F√≠sica com `.checkpoint()`:**

1. **`.drop()` expl√≠cito** ‚Üí Remove colunas Silver ANTES do `.select()`
2. **`.select()` espec√≠fico** ‚Üí Define schema Gold (14 colunas)
3. **`.checkpoint()`** ‚Üí **QUEBRA LINHAGEM** e materializa fisicamente em S3
4. **Valida√ß√£o fatal** ‚Üí Garante que n√£o h√° colunas Silver
5. **`writeTo()`** ‚Üí Usa DataFrame **sem linhagem** (apenas schema materializado)

**Diferen√ßa cr√≠tica:**
- **Antes:** `df_with_kpis (19 cols) ‚Üí .select() ‚Üí df_gold (14 cols logicamente)` ‚Üí Iceberg v√™ linhagem e usa 19 cols
- **Depois:** `df_with_kpis (19 cols) ‚Üí .drop() ‚Üí .select() ‚Üí .checkpoint()` ‚Üí **Linhagem quebrada** ‚Üí `df_gold (14 cols fisicamente)` ‚Üí Iceberg v√™ apenas 14 cols

---

## üìä Diagrama de Fluxo

### Fluxo ANTIGO (Falhava)
```
df_silver (19 cols com event_id)
    ‚Üì withColumn() [transforma√ß√µes l√≥gicas]
df_with_kpis (19 cols ainda)
    ‚Üì select() [transforma√ß√£o l√≥gica - n√£o quebra linhagem]
df_gold (14 cols LOGICAMENTE)
    ‚Üì writeTo().createOrReplace()
    ‚Üì [Iceberg inspeciona linhagem, v√™ df_with_kpis]
‚ùå Tabela criada com 19 colunas
```

### Fluxo NOVO (Deve Funcionar)
```
df_silver (19 cols com event_id)
    ‚Üì withColumn() [transforma√ß√µes l√≥gicas]
df_with_kpis (19 cols ainda)
    ‚Üì drop() [remove expl√≠citamente 5 cols]
df_cleaned (14 cols)
    ‚Üì select() [seleciona 14 cols espec√≠ficas]
df_gold (14 cols LOGICAMENTE)
    ‚Üì checkpoint() [QUEBRA LINHAGEM - materializa√ß√£o f√≠sica S3]
    ‚Üì [Grava parquet no S3, destroi hist√≥rico de transforma√ß√µes]
df_gold (14 cols FISICAMENTE - sem linhagem)
    ‚Üì writeTo().createOrReplace()
    ‚Üì [Iceberg recebe DataFrame sem linhagem, usa schema f√≠sico]
‚úÖ Tabela criada com 14 colunas
```

---

## üöÄ Pr√≥ximos Passos para Teste

### 1. Limpar Ambiente
```bash
# Deletar tabelas Gold existentes
aws glue delete-table --database-name "datalake_pipeline_catalog_dev" --name "gold_car_current_state_new"
aws glue delete-table --database-name "datalake_pipeline_catalog_dev" --name "fuel_efficiency_monthly"
aws glue delete-table --database-name "datalake_pipeline_catalog_dev" --name "performance_alerts_log_slim"

# Limpar S3 Gold
aws s3 rm "s3://datalake-pipeline-gold-dev/iceberg-warehouse/datalake_pipeline_catalog_dev.db/" --recursive

# Limpar checkpoints anteriores
aws s3 rm "s3://datalake-pipeline-glue-temp-dev/spark-checkpoints/" --recursive
```

### 2. Upload Scripts Corrigidos
```bash
cd c:\dev\HP\wsas\Poc

# Upload dos 3 jobs Gold corrigidos
aws s3 cp glue_jobs/gold_car_current_state_job_iceberg.py s3://datalake-pipeline-glue-scripts-dev/
aws s3 cp glue_jobs/gold_fuel_efficiency_job_iceberg.py s3://datalake-pipeline-glue-scripts-dev/
aws s3 cp glue_jobs/gold_performance_alerts_job_iceberg.py s3://datalake-pipeline-glue-scripts-dev/
```

### 3. Aplicar Terraform
```bash
cd terraform
terraform apply -target=aws_glue_job.gold_car_current_state_iceberg
terraform apply -target=aws_glue_job.gold_fuel_efficiency_iceberg
terraform apply -target=aws_glue_job.gold_performance_alerts_iceberg
```

### 4. Executar Workflow
```bash
# Aguardar 2 minutos para propaga√ß√£o S3
Start-Sleep -Seconds 120

# Iniciar workflow
aws glue start-workflow-run --name "datalake-pipeline-silver-gold-workflow-dev-eventdriven"
```

### 5. Monitorar Execu√ß√£o (10-15 minutos)
```bash
# Obter RunId
$runId = (aws glue get-workflow-runs --name "datalake-pipeline-silver-gold-workflow-dev-eventdriven" --max-results 1 --query "Runs[0].WorkflowRunId" --output text)

# Aguardar 10 minutos
Start-Sleep -Seconds 600

# Verificar status
aws glue get-workflow-run --name "datalake-pipeline-silver-gold-workflow-dev-eventdriven" --run-id $runId
```

---

## üìù Pontos de Aten√ß√£o no Log

### Sucesso Esperado:
```
‚úÖ Checkpoint directory set
‚úÖ Dropped 5 Silver-inherited columns: event_id, event_primary_timestamp, event_year, event_month, event_day
‚úÖ Checkpoint completed: 11 rows materialized
‚úÖ VALIDATION PASSED: No Silver-only columns in df_gold
Final column list (14 columns):
    1. car_chassis
    2. manufacturer
    ...
    14. gold_processing_timestamp
‚úÖ Table created/replaced successfully
```

### Falha (Schema validation):
```
‚ùå ERROR: Silver columns still present in df_gold: ['event_id', 'event_year']
ValueError: Silver columns still present in df_gold: ['event_id', 'event_year']
```

---

## üéØ Expectativa de Resultado

### Se FUNCIONAR ‚úÖ:
- Job Gold 1: **SUCCEEDED** (cria√ß√£o da tabela com 14 colunas)
- Job Gold 2: **SUCCEEDED** (depend√™ncia satisfeita)
- Job Gold 3: **SUCCEEDED** (pipeline completo)
- Tabelas Gold consult√°veis via Athena
- **Problema resolvido**

### Se FALHAR ‚ùå:
- Analisar CloudWatch Logs para ver qual valida√ß√£o falhou
- Verificar Spark UI para inspe√ß√£o do plano f√≠sico (Spark event logs em `s3://.../spark-ui-logs/`)
- Se erro persiste mesmo com checkpoint ‚Üí **Bug confirmado do Iceberg/Glue 4.0** ‚Üí Escalar AWS Support com evid√™ncias

---

## üì¶ Arquivos Modificados (Para Commit)

```
glue_jobs/
‚îú‚îÄ‚îÄ gold_car_current_state_job_iceberg.py    [MODIFICADO - checkpoint + valida√ß√£o]
‚îú‚îÄ‚îÄ gold_fuel_efficiency_job_iceberg.py       [MODIFICADO - checkpoint + valida√ß√£o]
‚îî‚îÄ‚îÄ gold_performance_alerts_job_iceberg.py    [MODIFICADO - checkpoint + valida√ß√£o]

terraform/
‚îî‚îÄ‚îÄ iceberg_migration.tf                      [MODIFICADO - spark-ui-logs path]

IMPLEMENTACAO_CORRECOES_GOLD.md              [NOVO - este documento]
```

---

## üîß Rollback (Se Necess√°rio)

Se as corre√ß√µes causarem novos problemas:

```bash
# Reverter para vers√£o anterior dos scripts
git checkout HEAD~1 glue_jobs/gold_*_iceberg.py

# Re-upload
aws s3 cp glue_jobs/gold_car_current_state_job_iceberg.py s3://datalake-pipeline-glue-scripts-dev/
aws s3 cp glue_jobs/gold_fuel_efficiency_job_iceberg.py s3://datalake-pipeline-glue-scripts-dev/
aws s3 cp glue_jobs/gold_performance_alerts_job_iceberg.py s3://datalake-pipeline-glue-scripts-dev/
```

---

**Preparado por:** GitHub Copilot AI Assistant  
**Data:** 13/11/2025 16:15 BRT  
**Vers√£o:** 1.0  
**Status:** PRONTO PARA TESTE
