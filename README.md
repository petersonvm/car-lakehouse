# üöó Car Lakehouse - Data Pipeline

**Pipeline de dados de telemetria veicular** implementado na AWS usando arquitetura Medallion (Landing ‚Üí Bronze ‚Üí Silver ‚Üí Gold) com orquestra√ß√£o via AWS Glue Workflow.

**Status**: ‚úÖ **PRODU√á√ÉO** - Pipeline 100% funcional e validado  
**Ambiente**: Development (dev)  
**√öltima Atualiza√ß√£o**: 06 de Novembro de 2025

---

## üìã Vis√£o Geral

### Arquitetura Medallion

Este projeto implementa um Data Lakehouse completo para processamento de telemetria veicular, seguindo a arquitetura Medallion:

- **üõ¨ Landing Zone**: Recebe dados brutos (JSON/CSV) de APIs, dispositivos IoT ou upload manual
- **ü•â Bronze Layer**: Armazena dados copiados do Landing em formato original (valida√ß√£o m√≠nima)
- **ü•à Silver Layer**: Dados limpos, padronizados (snake_case), deduplicated e particionados (Parquet)
- **ü•á Gold Layer**: Agrega√ß√µes de neg√≥cio prontas para consumo (dashboards, BI, APIs)

### Componentes Principais

| Componente | Quantidade | Descri√ß√£o |
|------------|------------|-----------|
| **Buckets S3** | 8 | Landing, Bronze, Silver, Gold, Scripts, Temp, Athena, Layers |
| **Lambda Functions** | 1 ativa | Ingestion (Landing ‚Üí Bronze) com S3 trigger autom√°tico |
| **Glue Jobs** | 4 ativos | 1 Silver + 3 Gold (car state, fuel efficiency, alerts) |
| **Glue Crawlers** | 6 ativos | Bronze, Silver, 3 Gold + schema discovery |
| **Glue Workflow** | 1 | Orquestra√ß√£o Silver‚ÜíGold (scheduled daily 02:00 UTC) |
| **Tabelas Catalog** | 5 | 1 Bronze + 1 Silver + 3 Gold |
| **IAM Roles** | 4 | Lambda, Glue Jobs Silver, Gold, Crawlers |

---

## üèóÔ∏è Arquitetura Completa

### Fluxo de Dados End-to-End

```mermaid
graph TB
    A[APIs/IoT/Manual] -->|PUT *.json,*.csv| B[S3 Landing Zone]
    B -->|S3 Event| C[Lambda Ingestion]
    C -->|COPY| D[S3 Bronze Layer]
    D -->|READ| E[Glue Job Silver]
    E -->|Flatten JSON<br/>Deduplicate<br/>Partition| F[S3 Silver Layer]
    F -->|CRAWL| G[Crawler Silver]
    G -->|UPDATE| H[Glue Catalog]
    H -->|TRIGGER Fan-Out| I{3 Jobs Gold}
    I -->|Parallel| J[Job Gold 1:<br/>Car Current State]
    I -->|Parallel| K[Job Gold 2:<br/>Fuel Efficiency]
    I -->|Parallel| L[Job Gold 3:<br/>Performance Alerts]
    J --> M[S3 Gold Layer]
    K --> M
    L --> M
    M --> N[Crawlers Gold]
    N --> H
    H --> O[Athena/BI Tools]
```

### Camadas de Dados

#### üìÇ Landing Zone
```
s3://datalake-pipeline-landing-dev/
‚îú‚îÄ‚îÄ *.json (raw telemetry data)
‚îî‚îÄ‚îÄ *.csv (batch uploads)

Status: TRANSIENT (arquivos removidos ap√≥s ingest√£o)
Trigger: Lambda Ingestion (autom√°tico via S3 Event)
```

#### üìÇ Bronze Layer
```
s3://datalake-pipeline-bronze-dev/
‚îî‚îÄ‚îÄ bronze/
    ‚îî‚îÄ‚îÄ car_data/
        ‚îî‚îÄ‚îÄ *.json (raw, 1:1 copy from Landing)

Tabela: car_bronze
Formato: JSON (original)
Tamanho: ~29 KB
```

#### üìÇ Silver Layer
```
s3://datalake-pipeline-silver-dev/
‚îî‚îÄ‚îÄ car_telemetry/
    ‚îî‚îÄ‚îÄ event_year=2025/
        ‚îî‚îÄ‚îÄ event_month=11/
            ‚îî‚îÄ‚îÄ event_day=05/
                ‚îî‚îÄ‚îÄ *.parquet (snappy compressed)

Tabela: silver_car_telemetry
Formato: Parquet (56 colunas snake_case)
Particionamento: event_year, event_month, event_day
Tamanho: ~13 KB
```

#### üìÇ Gold Layer
```
s3://datalake-pipeline-gold-dev/
‚îú‚îÄ‚îÄ gold_car_current_state_new/
‚îÇ   ‚îî‚îÄ‚îÄ *.parquet (1 row per vehicle, latest state)
‚îú‚îÄ‚îÄ fuel_efficiency_monthly/
‚îÇ   ‚îî‚îÄ‚îÄ *.parquet (aggregated by car+month)
‚îî‚îÄ‚îÄ gold_performance_alerts_slim/
    ‚îî‚îÄ‚îÄ *.parquet (alert logs)

Tabelas: gold_car_current_state_new, fuel_efficiency_monthly, performance_alerts_log_slim
Formato: Parquet
Tamanho Total: ~19 KB
```

---

## üì¶ Invent√°rio de Componentes

### 1. üóÑÔ∏è Glue Data Catalog

#### Database
| Nome | Catalog ID | Descri√ß√£o |
|------|------------|-----------|
| `datalake-pipeline-catalog-dev` | 901207488135 | Database principal do Lakehouse |

#### Tabelas (5 tabelas ativas)

##### Bronze
- **`bronze_car_data`**: Dados brutos (JSON) copiados do Landing
  - Localiza√ß√£o: `s3://datalake-pipeline-bronze-dev/bronze/car_data/`
  - Atualizada por: `datalake-pipeline-bronze-car-data-crawler-dev`

##### Silver
- **`silver_car_telemetry`**: Dados limpos e estruturados (56 colunas)
  - Localiza√ß√£o: `s3://datalake-pipeline-silver-dev/car_telemetry/`
  - Formato: Parquet particionado (event_year/month/day)
  - Atualizada por: `datalake-pipeline-silver-crawler-dev`

##### Gold
- **`gold_car_current_state_new`**: Estado atual de cada ve√≠culo (12 colunas)
  - Join de telemetria + dados est√°ticos + status de seguro
- **`fuel_efficiency_monthly`**: M√©tricas de efici√™ncia de combust√≠vel (7 colunas)
  - Agrega√ß√£o mensal por ve√≠culo (km/litro, dist√¢ncia, consumo)
- **`performance_alerts_log_slim`**: Log de alertas de performance
  - Anomalias baseadas em thresholds (temperatura, press√£o, bateria)

### 2. üöÄ Glue Jobs

#### Job Silver
**`datalake-pipeline-silver-consolidation-dev`**
- **Fun√ß√£o**: Bronze ‚Üí Silver (limpeza e estrutura√ß√£o)
- **Script**: `glue_jobs/silver_consolidation_job.py`
- **Workers**: 2 √ó G.1X (Glue 4.0)
- **Transforma√ß√µes**:
  - Flatten nested JSON (metrics.trip.* ‚Üí trip_*)
  - Convert camelCase ‚Üí snake_case (56 campos)
  - Deduplicate por event_id (Window function)
  - Particionar por data (year/month/day)
  - Adicionar processing_timestamp
- **Dura√ß√£o m√©dia**: 78s
- **Status**: ‚úÖ ATIVO (parte do workflow)

#### Jobs Gold (3 paralelos)

**1. `datalake-pipeline-gold-car-current-state-dev`**
- **Fun√ß√£o**: √öltimo estado de cada ve√≠culo
- **Transforma√ß√µes**:
  - Window: last_value over partition by car_chassis
  - Join telemetry + static info
  - Calcular status de seguro (v√°lido/expirado)
- **Dura√ß√£o m√©dia**: 91s

**2. `datalake-pipeline-gold-fuel-efficiency-dev`**
- **Fun√ß√£o**: Efici√™ncia de combust√≠vel mensal
- **Transforma√ß√µes**:
  - Extrair year/month de event_date
  - GroupBy car_chassis + year + month
  - Calcular: avg_fuel_efficiency (km/litro)
  - Somar: trip_distance_km, trip_fuel_consumed_liters
- **Dura√ß√£o m√©dia**: 93s

**3. `datalake-pipeline-gold-performance-alerts-slim-dev`**
- **Fun√ß√£o**: Alertas de performance
- **Transforma√ß√µes**:
  - Check thresholds (temp, press√£o, bateria)
  - Flag anomalias
  - Gerar alert records
- **Dura√ß√£o m√©dia**: 106s

### 3. üîç Glue Crawlers (6 ativos)

| Crawler | Camada | S3 Path | Tabela Criada |
|---------|--------|---------|---------------|
| `datalake-pipeline-bronze-car-data-crawler-dev` | Bronze | `bronze/car_data/` | `bronze_car_data` |
| `datalake-pipeline-silver-crawler-dev` | Silver | `car_telemetry/` | `silver_car_telemetry` |
| `gold_car_current_state_crawler` | Gold | `gold_car_current_state_new/` | `gold_car_current_state_new` |
| `datalake-pipeline-gold-fuel-efficiency-crawler-dev` | Gold | `fuel_efficiency_monthly/` | `fuel_efficiency_monthly` |
| `datalake-pipeline-gold-performance-alerts-slim-crawler-dev` | Gold | `gold_performance_alerts_slim/` | `performance_alerts_log_slim` |
| `gold_alerts_slim_crawler` | Gold | `performance_alerts_log_slim/` | `performance_alerts_log_slim` |

### 4. ü™£ Buckets S3 (8 buckets)

| Bucket | Camada | Prop√≥sito | Tamanho |
|--------|--------|-----------|---------|
| `datalake-pipeline-landing-dev` | Landing | Recebe uploads (JSON/CSV) | 0 bytes (transient) |
| `datalake-pipeline-bronze-dev` | Bronze | Raw data (1:1 copy) | ~29 KB |
| `datalake-pipeline-silver-dev` | Silver | Cleaned & partitioned | ~13 KB |
| `datalake-pipeline-gold-dev` | Gold | Business aggregations | ~19 KB |
| `datalake-pipeline-glue-scripts-dev` | Operacional | Job scripts (Python) | ~100 KB |
| `datalake-pipeline-glue-temp-dev` | Operacional | Temp files | Tempor√°rio |
| `datalake-pipeline-athena-results-dev` | Analytics | Query results | ~50 KB |
| `datalake-pipeline-lambda-layers-dev` | Operacional | Lambda layers | ~20 MB |

### 5. üîÑ Workflow Glue

**`datalake-pipeline-silver-gold-workflow-dev`**
- **Scheduled Start**: Di√°rio √†s 02:00 UTC (cron: `0 2 * * ? *`)
- **Dura√ß√£o m√©dia**: ~12 minutos
- **Actions**: 8 (1 job silver + 1 crawler silver + 3 jobs gold + 3 crawlers gold)
- **Status**: ‚úÖ 100% success rate (√∫ltima execu√ß√£o)

**Triggers (6 total):**
1. **Scheduled Start** ‚Üí Silver Job
2. **Silver Job SUCCEEDED** ‚Üí Silver Crawler
3. **Silver Crawler SUCCEEDED** ‚Üí Fan-Out (3 Gold Jobs em paralelo)
4. **Gold Job 1 SUCCEEDED** ‚Üí Gold Crawler 1
5. **Gold Job 2 SUCCEEDED** ‚Üí Gold Crawler 2
6. **Gold Job 3 SUCCEEDED** ‚Üí Gold Crawler 3

### 6. üîå Lambda Functions

**`datalake-pipeline-ingestion-dev`** (ATIVA)
- **Runtime**: Python 3.9 (512 MB, 120s timeout)
- **Trigger**: S3 Event (Landing bucket)
  - Eventos: `s3:ObjectCreated:*` (*.json, *.csv)
- **Fun√ß√£o**: Copiar Landing ‚Üí Bronze + cleanup Landing
- **Role**: `datalake-pipeline-lambda-execution-role-dev`
- **Status**: ‚úÖ ATIVA (√∫ltimas execu√ß√µes: 5 invoca√ß√µes)

> **Nota**: Pipeline completamente migrado para AWS Glue. Processamento Bronze‚ÜíSilver‚ÜíGold √© feito por Glue Jobs.

### 7. üîê IAM Roles (4 roles)

| Role | Usado Por | Principais Permiss√µes |
|------|-----------|------------------------|
| `datalake-pipeline-lambda-execution-role-dev` | Lambda Ingestion | S3 (Landing read, Bronze write), CloudWatch Logs |
| `datalake-pipeline-glue-job-role-dev` | Job Silver | S3 (Bronze read, Silver write), Glue Catalog, CloudWatch |
| `datalake-pipeline-gold-job-role-dev` | Jobs Gold (3) | S3 (Silver read, Gold write/delete), Glue Catalog |
| `datalake-pipeline-glue-crawler-role-dev` | Crawlers (6) | S3 read (all layers), Glue Catalog write |

---

## üîó Matriz de Comunica√ß√£o

| Origem | A√ß√£o | Destino | Dados Transferidos |
|--------|------|---------|-------------------|
| APIs/IoT/Manual | S3 PUT | Landing Bucket | JSON/CSV raw files |
| Landing Bucket | S3 Event | Lambda Ingestion | ObjectCreated trigger |
| Lambda Ingestion | S3 COPY | Bronze Bucket | JSON raw (1:1 copy) |
| Lambda Ingestion | S3 DELETE | Landing Bucket | Cleanup ap√≥s sucesso |
| Bronze Bucket | Glue READ | Job Silver | bronze_car_data table |
| Job Silver | S3 WRITE | Silver Bucket | Parquet particionado |
| Silver Bucket | Glue CRAWL | Crawler Silver | car_telemetry/ folders |
| Crawler Silver | Catalog UPDATE | Glue Catalog | silver_car_telemetry + partitions |
| Glue Catalog | Glue READ | Jobs Gold (3√ó) | silver_car_telemetry (56 cols) |
| Jobs Gold | S3 WRITE | Gold Bucket | gold_* tables (Parquet) |
| Gold Bucket | Glue CRAWL | Crawlers Gold (3√ó) | gold_*/ folders |
| Crawlers Gold | Catalog UPDATE | Glue Catalog | gold_* tables |
| Glue Catalog | Athena QUERY | Athena | SQL results ‚Üí Results Bucket |
| Workflow | TRIGGER | Job Silver | Scheduled (cron) |
| Workflow | TRIGGER | Crawler Silver | Conditional (Job SUCCEEDED) |
| Workflow | TRIGGER | Jobs Gold (3√ó) | Conditional (Crawler SUCCEEDED) |

---

## üìä Fluxo de Dados Detalhado

### Stage 1: Ingest√£o (Landing ‚Üí Bronze)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  External Sources      ‚îÇ
‚îÇ  ‚Ä¢ REST APIs           ‚îÇ
‚îÇ  ‚Ä¢ IoT Devices         ‚îÇ
‚îÇ  ‚Ä¢ Manual Upload       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ PUT *.json, *.csv
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  S3 Landing Zone       ‚îÇ
‚îÇ  Status: TRANSIENT     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ S3 ObjectCreated Event
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Lambda Ingestion               ‚îÇ
‚îÇ  ‚Ä¢ Validate file extension      ‚îÇ
‚îÇ  ‚Ä¢ COPY Landing ‚Üí Bronze         ‚îÇ
‚îÇ  ‚Ä¢ DELETE from Landing          ‚îÇ
‚îÇ  Duration: ~1-5s per file       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ S3 COPY
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  S3 Bronze Layer       ‚îÇ
‚îÇ  bronze/car_data/*.json‚îÇ
‚îÇ  Status: PERMANENT     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Stage 2: Consolida√ß√£o (Bronze ‚Üí Silver)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  S3 Bronze Layer       ‚îÇ
‚îÇ  Table: bronze_car_data‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ Glue READ (via Catalog)
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Glue Job: Silver Consolidation      ‚îÇ
‚îÇ  ‚Ä¢ Flatten JSON (nested ‚Üí flat)      ‚îÇ
‚îÇ  ‚Ä¢ Rename: camelCase ‚Üí snake_case    ‚îÇ
‚îÇ  ‚Ä¢ Deduplicate (Window + row_number) ‚îÇ
‚îÇ  ‚Ä¢ Add processing_timestamp          ‚îÇ
‚îÇ  ‚Ä¢ Partition by event_date           ‚îÇ
‚îÇ  ‚Ä¢ Convert to Parquet (Snappy)       ‚îÇ
‚îÇ  Duration: ~78s                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ S3 WRITE (Parquet)
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  S3 Silver Layer               ‚îÇ
‚îÇ  car_telemetry/                ‚îÇ
‚îÇ    event_year=YYYY/            ‚îÇ
‚îÇ      event_month=MM/           ‚îÇ
‚îÇ        event_day=DD/           ‚îÇ
‚îÇ          *.parquet             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ Glue CRAWL
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Glue Catalog                  ‚îÇ
‚îÇ  Table: silver_car_telemetry   ‚îÇ
‚îÇ  Schema: 56 cols (snake_case)  ‚îÇ
‚îÇ  Partitions: registered        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Stage 3: Agrega√ß√µes (Silver ‚Üí Gold)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Glue Catalog                  ‚îÇ
‚îÇ  silver_car_telemetry          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ Workflow TRIGGER (Fan-Out)
            ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚ñº             ‚ñº              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Gold    ‚îÇ  ‚îÇ Gold     ‚îÇ  ‚îÇ Gold        ‚îÇ
‚îÇ Job 1   ‚îÇ  ‚îÇ Job 2    ‚îÇ  ‚îÇ Job 3       ‚îÇ
‚îÇ Current ‚îÇ  ‚îÇ Fuel     ‚îÇ  ‚îÇ Alerts      ‚îÇ
‚îÇ State   ‚îÇ  ‚îÇ Effic.   ‚îÇ  ‚îÇ Slim        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ            ‚îÇ                ‚îÇ
     ‚îÇ WRITE      ‚îÇ WRITE          ‚îÇ WRITE
     ‚ñº            ‚ñº                ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  S3 Gold Layer                     ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ gold_car_current_state_new/   ‚îÇ
‚îÇ  ‚îú‚îÄ‚îÄ fuel_efficiency_monthly/      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ performance_alerts_slim/      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ Glue CRAWL (3√ó parallel)
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Glue Catalog (Gold Tables)        ‚îÇ
‚îÇ  ‚Ä¢ gold_car_current_state_new      ‚îÇ
‚îÇ  ‚Ä¢ fuel_efficiency_monthly         ‚îÇ
‚îÇ  ‚Ä¢ performance_alerts_log_slim     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ Athena QUERY
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Analytics & BI                    ‚îÇ
‚îÇ  ‚Ä¢ Athena SQL Queries              ‚îÇ
‚îÇ  ‚Ä¢ PowerBI / QuickSight            ‚îÇ
‚îÇ  ‚Ä¢ API Endpoints                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Estrutura do Projeto

```
.
‚îú‚îÄ‚îÄ terraform/                           # Infraestrutura como C√≥digo
‚îÇ   ‚îú‚îÄ‚îÄ provider.tf                      # Configura√ß√£o AWS Provider
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf                     # Defini√ß√£o de vari√°veis
‚îÇ   ‚îú‚îÄ‚îÄ s3.tf                            # Buckets S3 (8 buckets)
‚îÇ   ‚îú‚îÄ‚îÄ lambda.tf                        # Lambda Ingestion + Layer
‚îÇ   ‚îú‚îÄ‚îÄ iam.tf                           # Roles e Policies IAM (4 roles)
‚îÇ   ‚îú‚îÄ‚îÄ glue.tf                          # Crawler Bronze, Database Catalog
‚îÇ   ‚îú‚îÄ‚îÄ glue_silver.tf                   # Job Silver + Crawler Silver
‚îÇ   ‚îú‚îÄ‚îÄ glue_gold_car_current_state.tf   # Job Gold 1 + Crawler
‚îÇ   ‚îú‚îÄ‚îÄ glue_gold_fuel_efficiency.tf     # Job Gold 2 + Crawler
‚îÇ   ‚îú‚îÄ‚îÄ glue_gold_alerts_slim.tf         # Job Gold 3 + Crawler
‚îÇ   ‚îú‚îÄ‚îÄ glue_workflow.tf                 # Workflow + 6 Triggers
‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf                       # Outputs Terraform
‚îú‚îÄ‚îÄ glue_jobs/                           # Scripts Python dos Glue Jobs
‚îÇ   ‚îú‚îÄ‚îÄ silver_consolidation_job.py      # Bronze ‚Üí Silver (56 cols)
‚îÇ   ‚îú‚îÄ‚îÄ gold_car_current_state_job.py    # Silver ‚Üí Gold 1 (12 cols)
‚îÇ   ‚îú‚îÄ‚îÄ gold_fuel_efficiency_job.py      # Silver ‚Üí Gold 2 (7 cols)
‚îÇ   ‚îî‚îÄ‚îÄ gold_performance_alerts_slim_job.py  # Silver ‚Üí Gold 3 (alerts)
‚îú‚îÄ‚îÄ lambdas/                             # C√≥digo das Lambdas
‚îÇ   ‚îî‚îÄ‚îÄ ingestion/
‚îÇ       ‚îú‚îÄ‚îÄ lambda_function.py           # Lambda Ingestion (ativa)
‚îÇ       ‚îî‚îÄ‚îÄ README.md                    # Documenta√ß√£o t√©cnica
‚îú‚îÄ‚îÄ docs/                                # Documenta√ß√£o do Projeto
‚îÇ   ‚îî‚îÄ‚îÄ reports/                         # Relat√≥rios organizados
‚îÇ       ‚îú‚îÄ‚îÄ END_TO_END_TEST_REPORT.md
‚îÇ       ‚îú‚îÄ‚îÄ EXECUTIVE_SUMMARY.md
‚îÇ       ‚îú‚îÄ‚îÄ GOLD_LAYER_VALIDATION_REPORT.md
‚îÇ       ‚îú‚îÄ‚îÄ INVENTARIO_AWS.md
‚îÇ       ‚îú‚îÄ‚îÄ INVENTARIO_COMPONENTES_ATUALIZADO.md
‚îÇ       ‚îú‚îÄ‚îÄ INVENTARIO_INFRAESTRUTURA.md
‚îÇ       ‚îú‚îÄ‚îÄ RECOVERY_README.md
‚îÇ       ‚îú‚îÄ‚îÄ REFACTORING_SUMMARY.md
‚îÇ       ‚îú‚îÄ‚îÄ Relatorio_Componentes_Lakehouse.md
‚îÇ       ‚îî‚îÄ‚îÄ WORKFLOW_RECOVERY_GUIDE.md
‚îú‚îÄ‚îÄ scripts/                             # Scripts auxiliares
‚îú‚îÄ‚îÄ test_data/                           # Dados de teste
‚îú‚îÄ‚îÄ Data_Model/                          # Modelos de dados
‚îÇ   ‚îî‚îÄ‚îÄ car_raw.json                     # Schema exemplo
‚îú‚îÄ‚îÄ assets/                              # Arquivos auxiliares
‚îú‚îÄ‚îÄ build_lambda.ps1                     # Build Lambda (Windows)
‚îú‚îÄ‚îÄ build_lambda.sh                      # Build Lambda (Linux/Mac)
‚îú‚îÄ‚îÄ build_layer_docker.ps1               # Build Layer com Docker
‚îú‚îÄ‚îÄ terraform.tfvars                     # Vari√°veis Terraform (privado)
‚îú‚îÄ‚îÄ .gitignore                           # Git ignore rules
‚îú‚îÄ‚îÄ QUICK_REFERENCE.md                   # Refer√™ncia r√°pida
‚îî‚îÄ‚îÄ README.md                            # Este arquivo
```

---

## üöÄ Como Usar

### Pr√©-requisitos

1. **Terraform** >= 1.0
   ```bash
   terraform version
   ```

2. **AWS CLI** configurado
   ```bash
   aws configure
   ```

3. **Credenciais AWS** com permiss√µes para criar:
   - S3 Buckets, Lambda Functions, Glue (Jobs, Crawlers, Workflow)
   - IAM Roles e Policies, CloudWatch Log Groups

### Instala√ß√£o e Deploy

#### 1. Clonar o reposit√≥rio
```bash
git clone https://github.com/petersonvm/car-lakehouse.git
cd car-lakehouse
```

#### 2. Build da Lambda Ingestion (OBRIGAT√ìRIO antes do Terraform)

**Windows (PowerShell):**
```powershell
.\build_lambda.ps1
```

**Linux/Mac:**
```bash
chmod +x build_lambda.sh
./build_lambda.sh
```

Isso criar√°:
- `assets/ingestion_package.zip` (c√≥digo da Lambda)
- `assets/pandas_pyarrow_layer.zip` (Lambda Layer)

#### 3. Configurar vari√°veis

Navegue at√© o diret√≥rio Terraform:
```bash
cd terraform
```

Copie o arquivo de exemplo (se existir) ou edite diretamente `terraform.tfvars`:
```hcl
aws_region   = "us-east-1"
project_name = "datalake-pipeline"
environment  = "dev"

common_tags = {
  Project     = "Car-Lakehouse"
  ManagedBy   = "Terraform"
  Environment = "dev"
}
```

#### 4. Inicializar e aplicar Terraform

```bash
# Inicializar backend e providers
terraform init

# Validar configura√ß√£o
terraform validate

# Ver plano de execu√ß√£o
terraform plan

# Aplicar (criar recursos)
terraform apply
```

Digite `yes` quando solicitado.

#### 5. Verificar outputs

```bash
terraform output
```

Voc√™ ver√° informa√ß√µes sobre:
- Buckets S3 criados (8)
- Lambda Ingestion ARN
- Glue Database e tabelas
- Workflow ARN

---

## üß™ Testar o Pipeline

### 1. Upload de arquivo JSON para o Landing Zone

```bash
# Fazer upload de dados de teste
aws s3 cp test_data/sample_car_data.json s3://datalake-pipeline-landing-dev/

# Verificar logs da Lambda Ingestion
aws logs tail /aws/lambda/datalake-pipeline-ingestion-dev --follow
```

A Lambda ser√° invocada automaticamente e copiar√° o arquivo para Bronze.

### 2. Verificar arquivo no Bronze

```bash
aws s3 ls s3://datalake-pipeline-bronze-dev/bronze/car_data/ --recursive
```

### 3. Executar manualmente o Job Silver (opcional - ou esperar pelo scheduled trigger)

```bash
aws glue start-job-run --job-name datalake-pipeline-silver-consolidation-dev
```

### 4. Executar o Workflow completo

```bash
# Executar workflow manualmente
aws glue start-workflow-run --name datalake-pipeline-silver-gold-workflow-dev

# Verificar status
aws glue get-workflow-run --name datalake-pipeline-silver-gold-workflow-dev --run-id <RUN_ID>
```

O workflow executar√°:
1. Job Silver (78s)
2. Crawler Silver (atualizar parti√ß√µes)
3. 3 Jobs Gold em paralelo (91s, 93s, 106s)
4. 3 Crawlers Gold (atualizar tabelas)

**Dura√ß√£o total**: ~12 minutos

### 5. Consultar dados com Athena

```sql
-- Verificar tabela Silver
SELECT * FROM "datalake-pipeline-catalog-dev"."silver_car_telemetry"
LIMIT 10;

-- Verificar estado atual dos ve√≠culos
SELECT * FROM "datalake-pipeline-catalog-dev"."gold_car_current_state_new";

-- Verificar efici√™ncia de combust√≠vel
SELECT 
    car_chassis,
    year,
    month,
    avg_fuel_efficiency_km_per_liter,
    total_distance_km
FROM "datalake-pipeline-catalog-dev"."fuel_efficiency_monthly"
ORDER BY year DESC, month DESC;

-- Verificar alertas de performance
SELECT * FROM "datalake-pipeline-catalog-dev"."performance_alerts_log_slim"
WHERE alert_type = 'HIGH_TEMPERATURE'
LIMIT 100;
```

---

## üìä Schema dos Dados

### Silver Layer: `silver_car_telemetry` (56 colunas)

```python
# Identifica√ß√£o do Ve√≠culo
car_chassis: string
manufacturer: string
model: string
manufacturing_year: bigint
purchase_date: string

# Dados do Propriet√°rio
owner_name: string
owner_cpf: string
owner_email: string
owner_phone: string

# Seguro
insurance_company: string
insurance_policy_number: string
insurance_valid_until: string

# Telemetria Geral
telemetry_timestamp: timestamp
current_mileage_km: bigint
location_latitude: double
location_longitude: double
location_city: string
location_state: string

# Viagem (Trip)
trip_distance_km: double
trip_duration_minutes: bigint
trip_average_speed_km_h: double
trip_fuel_consumed_liters: double

# Motor (Engine)
engine_temperature_c: bigint
engine_rpm: bigint
engine_load_percent: bigint
engine_coolant_temp_c: bigint

# Bateria
battery_voltage_v: double
battery_charge_percent: bigint

# Pneus (Tires)
tire_pressure_front_left_psi: bigint
tire_pressure_front_right_psi: bigint
tire_pressure_rear_left_psi: bigint
tire_pressure_rear_right_psi: bigint

# Sensores
odometer_reading_km: bigint
fuel_level_percent: bigint
ambient_temperature_c: bigint

# Eventos e Alertas
event_id: string (PK)
event_date: string
event_year: bigint (partition)
event_month: bigint (partition)
event_day: bigint (partition)

# Metadados
processing_timestamp: timestamp
```

### Gold Layer 1: `gold_car_current_state_new` (12 colunas)

```python
car_chassis: string
manufacturer: string
model: string
manufacturing_year: bigint
owner_name: string
current_mileage_km: bigint
fuel_level_percent: bigint
battery_voltage_v: double
insurance_company: string
insurance_valid_until: string
insurance_status: string  # "VALID" or "EXPIRED"
last_telemetry_timestamp: timestamp
```

### Gold Layer 2: `fuel_efficiency_monthly` (7 colunas)

```python
car_chassis: string
year: bigint
month: bigint
total_distance_km: double
total_fuel_consumed_liters: double
number_of_trips: bigint
avg_fuel_efficiency_km_per_liter: double
```

---

## üîê Seguran√ßa

- **‚úÖ Criptografia**: Todos os buckets S3 usam AES256
- **‚úÖ Acesso P√∫blico**: Bloqueado por padr√£o em todos os buckets
- **‚úÖ Versionamento**: Habilitado nos buckets principais
- **‚úÖ IAM**: Princ√≠pio do menor privil√©gio (least privilege)
- **‚úÖ CloudWatch Logs**: Habilitado para todas as execu√ß√µes
- **‚úÖ Job Bookmarks**: Habilitado no Job Silver (evita reprocessamento)

---

## üìà Monitoramento e Observabilidade

### CloudWatch Log Groups

```bash
# Lambda Ingestion
/aws/lambda/datalake-pipeline-ingestion-dev

# Glue Jobs
/aws/glue/jobs/datalake-pipeline-silver-consolidation-dev
/aws/glue/jobs/datalake-pipeline-gold-car-current-state-dev
/aws/glue/jobs/datalake-pipeline-gold-fuel-efficiency-dev
/aws/glue/jobs/datalake-pipeline-gold-performance-alerts-slim-dev

# Crawlers
/aws/glue/crawlers
```

### M√©tricas Principais

| M√©trica | Namespace | Descri√ß√£o |
|---------|-----------|-----------|
| `Duration` | Lambda | Tempo de execu√ß√£o da Lambda Ingestion |
| `Errors` | Lambda | Erros na Lambda Ingestion |
| `glue.driver.aggregate.numCompletedStages` | Glue | Est√°gios completados nos Jobs |
| `glue.driver.aggregate.numFailedTasks` | Glue | Tarefas falhadas nos Jobs |

### Consultar Execu√ß√µes Recentes

```bash
# Workflow runs
aws glue get-workflow-runs --name datalake-pipeline-silver-gold-workflow-dev --max-results 5

# Job runs (Silver)
aws glue get-job-runs --job-name datalake-pipeline-silver-consolidation-dev --max-results 5

# Crawler runs
aws glue get-crawler-metrics --crawler-name-list datalake-pipeline-silver-crawler-dev
```

---

## üí∞ Estimativa de Custos (Desenvolvimento)

| Servi√ßo | Uso Mensal | Custo Estimado |
|---------|------------|----------------|
| **S3 Storage** | ~200 KB | < $0.01 |
| **Lambda Invocations** | ~100 invocations | < $0.01 |
| **Glue Jobs** | 30 runs √ó 4 jobs √ó 2 min | ~$1.20 |
| **Glue Crawlers** | 30 runs √ó 6 crawlers | ~$0.50 |
| **Athena Queries** | ~1 TB scanned | ~$5.00 |
| **CloudWatch Logs** | 1 GB | ~$0.50 |
| **Total Estimado** | | **~$7.21/m√™s** |

> **Nota**: Custos reais variam conforme o volume de dados e frequ√™ncia de execu√ß√£o.

---

## üßπ Otimiza√ß√£o de Infraestrutura

### Limpeza de Recursos Legados

O pipeline atual cont√©m ~10 recursos legados (√≥rf√£os de refatora√ß√µes anteriores) que podem ser removidos para otimiza√ß√£o de custos:

**Recursos Identificados:**
- 3 Lambdas n√£o utilizadas (cleansing, analysis, compliance)
- 2 Crawlers duplicados (gold_alerts_slim, gold_fuel_efficiency)
- 1 IAM Role √≥rf√£ + 3 policies associadas

**Economia Estimada:** ~$0.50/m√™s + redu√ß√£o de 7% na complexidade do Terraform

**Como Executar a Limpeza:**

```bash
# 1. Revisar plano detalhado
cat docs/TERRAFORM_CLEANUP_PLAN.md

# 2. Simular limpeza (DRY RUN - n√£o faz altera√ß√µes)
.\scripts\cleanup_legacy_resources.ps1 -DryRun

# 3. Executar limpeza REAL (com backup autom√°tico)
.\scripts\cleanup_legacy_resources.ps1

# 4. Validar pipeline ap√≥s limpeza
aws glue start-workflow-run --name datalake-pipeline-silver-gold-workflow-dev
```

**Documenta√ß√£o Completa:**
- **[docs/TERRAFORM_CLEANUP_PLAN.md](./docs/TERRAFORM_CLEANUP_PLAN.md)**: Plano detalhado de limpeza
- **[scripts/cleanup_legacy_resources.ps1](./scripts/cleanup_legacy_resources.ps1)**: Script automatizado

> **‚úÖ Seguro**: Script inclui backup autom√°tico do Terraform state e modo DRY RUN para simula√ß√£o.

---

## üóëÔ∏è Destruir Recursos (Remover Tudo)

Para remover **TODOS** os recursos criados:

```bash
cd terraform
terraform destroy
```

‚ö†Ô∏è **ATEN√á√ÉO**: 
- Fa√ßa backup dos dados S3 antes de destruir!
- Buckets com versionamento requerem remo√ß√£o manual de todas as vers√µes

---

## üìö Documenta√ß√£o Adicional

Para informa√ß√µes mais detalhadas, consulte:

- **[QUICK_REFERENCE.md](./QUICK_REFERENCE.md)**: Comandos r√°pidos e refer√™ncias
- **[docs/TERRAFORM_CLEANUP_PLAN.md](./docs/TERRAFORM_CLEANUP_PLAN.md)**: üßπ Plano de limpeza de recursos legados
- **[docs/reports/INVENTARIO_COMPONENTES_ATUALIZADO.md](./docs/reports/INVENTARIO_COMPONENTES_ATUALIZADO.md)**: Invent√°rio completo detalhado
- **[docs/reports/END_TO_END_TEST_REPORT.md](./docs/reports/END_TO_END_TEST_REPORT.md)**: Relat√≥rio de testes end-to-end
- **[docs/reports/WORKFLOW_RECOVERY_GUIDE.md](./docs/reports/WORKFLOW_RECOVERY_GUIDE.md)**: Guia de recupera√ß√£o do workflow
- **[test_data/README.md](./test_data/README.md)**: üß™ Guia de dados de teste

---

## üõ†Ô∏è Troubleshooting

### Lambda Ingestion n√£o √© invocada

1. Verificar se S3 Event Notifications est√£o configurados:
   ```bash
   aws s3api get-bucket-notification-configuration --bucket datalake-pipeline-landing-dev
   ```

2. Verificar permiss√µes da Lambda:
   ```bash
   aws lambda get-policy --function-name datalake-pipeline-ingestion-dev
   ```

### Job Silver falha

1. Verificar se tabela Bronze existe:
   ```bash
   aws glue get-table --database-name datalake-pipeline-catalog-dev --name bronze_car_data
   ```

2. Verificar logs do Job:
   ```bash
   aws logs tail /aws/glue/jobs/datalake-pipeline-silver-consolidation-dev --follow
   ```

### Workflow n√£o inicia

1. Verificar se trigger est√° habilitado:
   ```bash
   aws glue get-triggers --query "Triggers[?WorkflowName=='datalake-pipeline-silver-gold-workflow-dev']"
   ```

2. Iniciar manualmente:
   ```bash
   aws glue start-workflow-run --name datalake-pipeline-silver-gold-workflow-dev
   ```

---

## ü§ù Contribui√ß√£o

Contribui√ß√µes s√£o bem-vindas! Por favor:
1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudan√ßas (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

---

## üìÑ Licen√ßa

Este projeto √© fornecido como exemplo educacional para demonstra√ß√£o de arquitetura de Data Lakehouse na AWS.

---

## üë• Autores

- **Peterson VM** - [GitHub](https://github.com/petersonvm)

---

## üôè Agradecimentos

- AWS Glue Documentation
- Databricks Medallion Architecture
- Terraform AWS Provider Community

---

**Desenvolvido com ‚ù§Ô∏è usando Terraform, AWS Glue e Python**
